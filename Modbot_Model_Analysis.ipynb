{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using vader, roberta pretrained model and hugging face pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data and NLTK basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\RajBu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\RajBu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\RajBu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "import random\n",
    "import nltk\n",
    "nltk.download('punkt')  # Ensure the standard punkt tokenizer is installed\n",
    "nltk.download('averaged_perceptron_tagger')  # Sometimes required for tokenization\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45615, 2)\n",
      "                                                  text  label\n",
      "0    \"QT @user In the original draft of the 7th boo...      2\n",
      "1    \"Ben Smith / Smith (concussion) remains out of...      1\n",
      "2    Sorry bout the stream last night I crashed out...      1\n",
      "3    Chase Headley's RBI double in the 8th inning o...      1\n",
      "4    @user Alciato: Bee will invest 150 million in ...      2\n",
      "..                                                 ...    ...\n",
      "495  \"Your 3rd gen. iPad with Retina display is wor...      1\n",
      "496  \"From the Twitter just now from RNDM: \"\"\"\"RNDM...      1\n",
      "497  @user @user You are a Patriot! Such compassion...      2\n",
      "498  \"Watching Les Mis 25th Anniversary concert. Ho...      0\n",
      "499  I\\u2019d want Mannone\\u002cJenks/sagna\\u002cpe...      1\n",
      "\n",
      "[500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('sentiment_train.csv')\n",
    "print(df.shape)\n",
    "print(df.head(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1AAAAHWCAYAAAB5bWjdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAABHH0lEQVR4nO3de1hVZf7//9eGvQUUYSuIQMjBlLQk0CmbPISZKaXjIUsdarJM/XjpOM3VNI3fRlMbnCKn+WSXlY3iaeqjlY2HPJdp6WBjmWdNUtyKCrEZ2ZTiAWT9/ujHyh2oCwO3xvNxXV6y1rr3Wu97s7vdr+51sBmGYQgAAAAAcFl+vi4AAAAAAK4XBCgAAAAAsIgABQAAAAAWEaAAAAAAwCICFAAAAABYRIACAAAAAIsIUAAAAABgEQEKAAAAACwiQAEAAACARQQoAMB1Ze3aterUqZOcTqdsNpv69+9/yfZz586VzWbT3Llzr0p9AICfNwIUAPjQV199pbFjx6pdu3YKDQ1VgwYNFB0drd69eysrK0tnz571dYmXdTUDisvlUr9+/XTo0CENGzZMEydO1JAhQ+r8uJezYcMG2Ww2TZo0ydelXHcee+wx2Ww2uVwuX5cCAJbYfV0AANRXzz//vCZPnqyKigrdeeedGjp0qIKDg/XNN99ow4YNGj58uN544w198cUXvi71mvHRRx/pzJkzevnll5Wenu7rcgAA9RABCgB84K9//asmTpyoFi1a6L333tMdd9xRpc3y5cv18ssv+6C6a9fx48clSdHR0T6uBABQX3EKHwBcZS6XS5MmTZLD4dDKlSurDU+S1KdPH61evbrK+nfffVd33XWXQkNDFRQUpKSkJL3wwgvVnu5ns9nUrVu3avdf3alTLpdLNptNjz32mFwul4YMGaLw8HAFBgbqtttu0/Lly7320a1bNz3++OOSpMcff1w2m838Y/WULCv9qTxFbuLEiZKku+++2zzOhg0bLB1HktavX69u3bqpcePGCgkJUe/evbVv374q7XJycjRu3DjddtttatasmQICAhQXF6eRI0fq6NGjXm0fe+wx3X333ZKkyZMne70HP65twYIFuvvuu+V0OhUYGKi2bdsqIyOjxqdq5ufna8yYMYqPj1eDBg3UrFkzPfDAA9q6dWuVtheeYrl69Wp169ZNoaGhstlslz3OhZ+RN998U0lJSQoMDFTz5s01cuRIlZSUVPu6rVu3auDAgYqIiDDfu9GjRys/P9+rnc1m07x58yRJCQkJ5vsWHx9fo/cDAK4mZqAA4CqbM2eOysrKNGTIELVr1+6SbQMCAryWn332Wb3wwgsKDw9Xenq6goODtWrVKj377LNas2aN1q5dqwYNGvzkGg8fPqyOHTuqZcuW+s1vfqMTJ07onXfeUb9+/fTRRx+ZgeGxxx6T0+nU0qVL1a9fP6WkpJj7cDqdlz2O1f7Ex8dr4sSJ2rBhgz755BMNHTrU/JJt9cv28uXLtXTpUt13330aNWqU9u7dq5UrV+rzzz/X3r17FR4ebrb917/+pRkzZujuu+9Wp06d1KBBA+3Zs0ezZs3SBx98oC+++EI33HCDJJk3sZg3b55SU1O9AuuFtQ0bNkxz5sxRTEyMBg4cKKfTqc8++0wTJkzQunXr9OGHH8puv/w/y4cOHVKXLl10/Phxde/eXb/+9a+Vl5en9957TytWrND777+vPn36VHndokWLtHr1arP/hw8ftvS+SdIzzzyjNWvW6Fe/+pV69uyp9evXa+bMmTpw4IA+/vjjKu/zwIEDZRiGHnzwQcXFxWnr1q164403tHTpUm3atEkJCQmSpIkTJ2rJkiXasWOHnnzySfMzY+WzAwA+YwAArqru3bsbkoyZM2fW6HXZ2dmGJKNFixZGfn6+ub6srMzo06ePIcmYMmWK12skGampqdXub+jQoYYk49ChQ+a6Q4cOGZIMScakSZO82q9evdqQZNx3331e6+fMmWNIMubMmVPn/Zk4caIhyVi/fr3l41TW5+/vb3z00Ude28aNG2dIMjIzM73WHz161Dhz5kyVfa1Zs8bw8/MzRo0a5bV+/fr1hiRj4sSJl6xhwIABRmlpabV9euWVVyz1p2fPnoYkIyMjw2v9v//9b8Pf399o2rSp8d1331U5ts1mM1atWmXpGJUqPyMtWrQwDh8+bK4vKyszunbtakgy/vOf/5jrv/vuO6Np06aGn5+f8emnn3rt68UXXzQkGffee2+1x7jwcwgA1zJO4QOAq6zyNKaYmJgavW727NmSpPHjxysyMtJcb7fb9fLLL8vPz0+zZs2qlRrj4uI0fvx4r3W9evVSbGystmzZUivHuJr9kaQhQ4bonnvu8Vo3cuRISarSpxtuuKHK7J8k9ezZU7fccovWrFlTo2NPmzZNdrtds2fPVlBQkNe2CRMmKCwsTG+//fZl93P06FGtXbtWsbGxeuaZZ7y2derUSb/+9a914sQJ/etf/6ry2n79+iktLa1GdVd67rnnFBsbay7b7Xbz1M0L37ulS5fqxIkTGjx4sLp27eq1jz/84Q+Kj4/Xhx9+qCNHjlxRHQBwLeAUPgC4Tnz55ZeSpO7du1fZlpiYqJiYGB06dEglJSUKDQ39ScdKSUmRv79/lfUtWrTQ5s2bf9K+K13N/kjSbbfdVmVdixYtJEnFxcVe6w3D0Ntvv625c+dqx44dKi4u1vnz583tNTlNsrS0VDt27FB4eLheeeWVatsEBARUey3Wj23btk2S1LVrVzkcjirbu3fvrrfeekvbtm3To48+6rWtY8eOlmv+Mavv3aV+p3a7XXfddZdcLpe2bdvmFcgA4HpCgAKAqywqKkr79u3TsWPHavS6ygv2o6KiLrrfI0eOyOPx/OTAcbFrUOx2uyoqKn7Svitdzf5I1fep8pqjC8ORJD311FN65ZVXFBUVpV69eumGG24wZ47mzp1bo+uHiouLZRiG3G63Jk+efOUdkLX3TJI8Hk+VbRfO8tWU1ffup9QHANcLAhQAXGVdunTRxx9/rHXr1umJJ56w/LrKEFFQUKAbb7yxyvbKUwMvDBs2m03l5eXV7s/XX2KvpD9XQ2FhoV599VW1a9dO2dnZaty4sdf2BQsW1Gh/lfW3b9/enKG5Uhe+Z9W51Htm5a57P9VPqQ8ArhdcAwUAV9njjz8uh8Oh999/X3v37r1k2wtvb92+fXtJqva23QcOHNDRo0eVkJDgNVvQpEkT5eXlVWl//vx5bd++/Yrq/7HKU/1+PItzOVfSn6shNzdXFRUV6tmzZ5XwdPToUeXm5lZ5zaXeg+DgYN1yyy3as2ePTpw48ZNqq3zPNm3aVG0wXr9+vSSpQ4cOP+k4V+pSv9Py8nJt3LhRknd9V/r5AQBfIUABwFUWHx+vSZMm6dy5c+rdu7e++OKLattV3nK60rBhwyRJGRkZcrvd5vrz58/r6aefVkVFRZUZrY4dO+rIkSNau3at1/qMjIwanYZ2KWFhYZJU4xsDXEl/robKW49v2rTJ60v9yZMnNWLEiGqDy+Xeg6eeekrnzp3TsGHDqp35Ky4utjQ7FRMTo3vvvVcul6vK9VT/+c9/9H//939q0qSJBgwYcNl91YX+/furadOmWrBggT777DOvba+88ooOHTqkHj16eF3/dKWfHwDwFU7hAwAfePbZZ1VeXq7Jkyfr9ttvV6dOnXTbbbcpODhY33zzjT799FN9/fXXXhfvd+rUSc8884xeeukltWvXTg8++KAaNWqkVatWaffu3erSpYv++Mc/eh3n6aef1po1a9SvXz8NHjxYTZs2VXZ2tg4dOqRu3brV6CG0F3PnnXeqYcOGeuWVV/Tf//7XvNZm7NixlzxV60r6czVERkZqyJAhWrhwoVJSUtSzZ0+VlJToww8/VGBgoFJSUqrM3t1000264YYbtHDhQjkcDsXFxclms+k3v/mN4uLiNGzYMG3dulWvv/66brzxRvOOhidOnNChQ4f06aef6vHHH9eMGTMuW9+MGTPUuXNn/fGPf9TatWt12223mc+B8vPz05w5c6rMnF0twcHBmj17th566CGlpqbqoYceUmxsrLZu3aq1a9cqMjJSb775ptdr7rnnHk2dOlUjRozQwIED1bhxYzmdTv32t7/1SR8A4LJ8fR91AKjP9u7da/z2t781brnlFqNx48aGw+EwIiMjjbS0NGPWrFnVPotowYIFRufOnY3g4GAjICDAuPnmm42MjAzj9OnT1R5j6dKlxi9+8QsjICDAaNq0qTF48GDD5XJd8jlQQ4cOrXZfqampRnX/dKxatcr45S9/aTRq1Mh8jpTV5/rUpD8/5TlQF3tOlap5VtapU6eMZ5991rjxxhuNgIAAIyYmxhg9erRRVFR00fdgy5YtRvfu3Y2QkBDDZrNVW+cHH3xg9O7d22jWrJnhcDiM5s2bG7fffrvx5z//2di3b5/lPh09etQYNWqUERsbazgcDiMsLMzo16+fsWXLlhr3/1Iu9YymSz37asuWLUb//v2N8PBww+FwGC1atDBGjRplHDt2rNrjvPzyy0abNm2MBg0aGJKMuLi4GtcKAFeLzTAMwxfBDQAAAACuN1wDBQAAAAAWEaAAAAAAwCICFAAAAABYRIACAAAAAIsIUAAAAABgEQEKAAAAACwiQAEAAACARQQoAAAAALDI7usCrgXFxcUqLy/3dRnwkWbNmsntdvu6DAA+xDgAgHEAdrtdTZo0uXy7q1DLNa+8vFxlZWW+LgM+YLPZJH3/GTAMw8fVAPAFxgEAjAOoCU7hAwAAAACLCFAAAAAAYBEBCgAAAAAsIkABAAAAgEUEKAAAAACwiAAFAAAAABYRoAAAAADAIgIUAAAAAFhEgAIAAAAAiwhQAAAAAGARAQoAAAAALCJAAQAAAIBFBCgAAAAAsIgABQAAAAAWEaAAAAAAwCK7rwsAAPje+RF9fV2CT+X5uoBrgP/MZb4uAQCuC8xAAQAAAIBFBCgAAAAAsIgABQAAAAAWEaAAAAAAwCICFAAAAABYRIACAAAAAIsIUAAAAABgUY2eA7V48WJt2bJFx44dU4MGDZSYmKhHHnlE0dHRZptz585p/vz5ys7OVllZmZKTkzV8+HA5nU6zTVFRkWbOnKk9e/YoMDBQqampSk9Pl7+/v9lmz549mj9/vvLy8hQWFqaBAweqW7duXvWsXr1aH3zwgTwej+Li4jRs2DC1atXqyt4JAAAAALiMGs1A7d27V7169dKUKVM0fvx4nT9/XhkZGTpz5ozZZt68edq6daueeuopTZ48WcXFxXr55ZfN7RUVFXrhhRdUXl6ujIwMjRkzRhs2bNA777xjtiksLNSLL76oW265RS+99JJ69+6tGTNmaPv27Wab7OxszZ8/Xw8++KAyMzMVFxenKVOmqKSk5Ce8HQAAAABwcTUKUH/+85/VrVs3tWjRQvHx8RozZoyKioqUm5srSSotLdXHH3+soUOHql27dmrZsqVGjx6t/fv3KycnR5K0Y8cOHT16VGPHjlV8fLzat2+vwYMHa82aNSovL5ckrV27VhEREXr00UcVExOjtLQ0/fKXv9SKFSvMWpYvX6577rlHd999t2JiYjRixAg1aNBA69evr633BgAAAAC81OgUvh8rLS2VJAUHB0uScnNzdf78eSUlJZltbrjhBoWHhysnJ0eJiYnKyclRbGys1yl9KSkpmjVrlvLy8pSQkKCvv/7aax+SlJycrLlz50qSysvLlZubq/79+5vb/fz8lJSUZAa16pSVlamsrMxcttlsCgoKMn9G/VP5e+f3D6C+YxxEfcb3AdTEFQeoiooKzZ07VzfddJNiY2MlSR6PR3a7XY0aNfJqGxoaKo/HY7a5MDxVbq/cVvl35boL25w+fVrnzp3TyZMnVVFRUWU/TqdTx48fv2jNixcv1qJFi8zlhIQEZWZmqlmzZla7jZ+pyMhIX5cA+FSerwuAz0VFRfm6BMDn+D4AK644QGVlZSkvL0/PP/98bdZTpwYMGKA+ffqYy5X/l8HtdpunD6J+sdlsioyMVEFBgQzD8HU5AOAz+fn5vi4B8Bm+D0CS7Ha7pYmVKwpQWVlZ+vLLLzV58mSFhYWZ651Op8rLy3Xq1CmvWaiSkhJztsjpdOrAgQNe+6u88cOFbX58M4iSkhIFBQWpQYMGCgkJkZ+fnzljVam62a0LORwOORyOarfxH0v9ZhgGnwEA9RpjIMD3AVhTo5tIGIahrKwsbdmyRc8995wiIiK8trds2VL+/v7atWuXue748eMqKipSYmKiJCkxMVFHjhzxCkg7d+5UUFCQYmJiJEmtW7f22kdlm8p92O12tWzZUrt37za3V1RUaPfu3WYbAAAAAKhtNQpQWVlZ2rhxo5588kkFBQXJ4/HI4/Ho3LlzkqSGDRuqe/fumj9/vnbv3q3c3Fy9/vrrSkxMNINNcnKyYmJiNH36dLlcLm3fvl0LFy5Ur169zNmhnj17qrCwUG+99ZaOHTumNWvWaPPmzerdu7dZS58+fbRu3Tpt2LBBR48e1axZs3T27Nkqz4oCAAAAgNpiM2owTzlo0KBq148ePdoMLpUP0v33v/+t8vLyah+k63a7NWvWLO3Zs0cBAQFKTU3Vww8/XOVBuvPmzdPRo0cv+SDdZcuWyePxKD4+Xo8//rhat25tvfcX1HPh3flQf9hsNkVFRSk/P58pe9Rr50f09XUJ8DH/mct8XQLgM3wfgPT95T5WroGqUYD6uSJA1V8MmMD3CFAgQKE+4/sAJOsBqkan8AEAAABAfUaAAgAAAACLCFAAAAAAYBEBCgAAAAAsIkABAAAAgEUEKAAAAACwiAAFAAAAABYRoAAAAADAIgIUAAAAAFhEgAIAAAAAiwhQAAAAAGARAQoAAAAALCJAAQAAAIBFBCgAAAAAsIgABQAAAAAWEaAAAAAAwCICFAAAAABYRIACAAAAAIsIUAAAAABgEQEKAAAAACwiQAEAAACARQQoAAAAALCIAAUAAAAAFhGgAAAAAMAiAhQAAAAAWESAAgAAAACLCFAAAAAAYBEBCgAAAAAsIkABAAAAgEUEKAAAAACwiAAFAAAAABbZfV0AAAAAfO/8iL6+LsGn8nxdwDXAf+YyX5dwXWAGCgAAAAAsqvEM1N69e7Vs2TIdOnRIxcXFevrpp9WxY0dz+6BBg6p93SOPPKK+fb//PxtjxoyR2+322p6enq7+/fuby4cPH1ZWVpYOHjyokJAQpaWlqV+/fl6v2bx5s9555x253W5FRkbq4YcfVocOHWraJQAAAACwpMYB6uzZs4qPj1f37t31t7/9rcr2f/zjH17L27Zt04wZM3THHXd4rR80aJB69OhhLgcGBpo/l5aWKiMjQ0lJSRoxYoSOHDmiN954Q40aNTJfs3//fk2bNk3p6enq0KGDNm3apKlTpyozM1OxsbE17RYAAAAAXFaNA1T79u3Vvn37i253Op1ey59//rluueUWNW/e3Gt9UFBQlbaVNm3apPLyco0ePVp2u10tWrSQy+XS8uXLzQC1cuVKpaSkmLNaQ4YM0a5du7R69WqNHDmypt0CAAAAgMuq05tIeDwebdu2TWPGjKmybcmSJXr//fcVHh6uLl26qHfv3vL395ck5eTkqG3btrLbfygvOTlZS5cu1cmTJxUcHKycnBz16dPHa5/Jycn6/PPPL1pPWVmZysrKzGWbzaagoCDzZ9Q/lb93fv8A6jvGQQCMA9bUaYD65JNPFBgY6HWNlCTdd999SkhIUHBwsPbv368FCxaouLhYQ4cOlfR98IqIiPB6TeVslcfjUXBwsDwej0JDQ73ahIaGyuPxXLSexYsXa9GiReZyQkKCMjMz1axZs5/QS/wcREZG+roEwKe4+xSioqJ8XQJ8jHEAjAPW1GmAWr9+vbp27aoGDRp4rb9w5iguLk52u10zZ85Uenq6HA5HndUzYMAAr2NXpmy3263y8vI6Oy6uXTabTZGRkSooKJBhGL4uBwB8Jj8/39clAPCx+j4O2O12SxMrdRag9u3bp+PHj+v3v//9Zdu2bt1a58+fl9vtVnR0tJxOZ5WZpMrlypkop9OpkpISrzYlJSUXva5KkhwOx0UDGl+e6zfDMPgMAKjXGAMBMA5YU2fPgfr444/VsmVLxcfHX7aty+WSzWZTSEiIJCkxMVH79u3zmhXauXOnoqOjFRwcbLbZtWuX13527typ1q1b114nAAAAAOACNQ5QZ86ckcvlksvlkiQVFhbK5XKpqKjIbFNaWqrPPvtM3bt3r/L6nJwcrVixQi6XS9988402btyoefPmqWvXrmY46tKli+x2u2bMmKG8vDxlZ2dr1apVXqff3X///dqxY4c++OADHTt2TO+++64OHjyotLS0mnYJAAAAACyp8Sl8Bw8e1OTJk83l+fPnS5JSU1PNu+1lZ2fLMAx16dKl6gHtdmVnZ+u9995TWVmZIiIi1Lt3b69w1LBhQ40fP15ZWVkaN26cGjdurIEDB3o9N+qmm27S7373Oy1cuFALFixQVFSU/vjHP/IMKAAAAAB1xmZwsqPcbrfX7c1Rf9hsNkVFRSk/P5/zflGvnR/R19clwMf8Zy7zdQnwMcYB1PdxwOFwWLqJRJ1dAwUAAAAAPzcEKAAAAACwiAAFAAAAABYRoAAAAADAIgIUAAAAAFhEgAIAAAAAiwhQAAAAAGARAQoAAAAALCJAAQAAAIBFBCgAAAAAsIgABQAAAAAWEaAAAAAAwCICFAAAAABYRIACAAAAAIsIUAAAAABgEQEKAAAAACwiQAEAAACARQQoAAAAALCIAAUAAAAAFhGgAAAAAMAiAhQAAAAAWESAAgAAAACLCFAAAAAAYBEBCgAAAAAsIkABAAAAgEUEKAAAAACwiAAFAAAAABYRoAAAAADAIgIUAAAAAFhEgAIAAAAAiwhQAAAAAGARAQoAAAAALCJAAQAAAIBFBCgAAAAAsMhe0xfs3btXy5Yt06FDh1RcXKynn35aHTt2NLe/9tpr+uSTT7xek5ycrD//+c/m8smTJzV79mxt3bpVNptNd9xxhx5//HEFBgaabQ4fPqysrCwdPHhQISEhSktLU79+/bz2u3nzZr3zzjtyu92KjIzUww8/rA4dOtS0SwAAAABgSY0D1NmzZxUfH6/u3bvrb3/7W7VtUlJSNHr06B8OYvc+zKuvvqri4mKNHz9e58+f1+uvv64333xTTz75pCSptLRUGRkZSkpK0ogRI3TkyBG98cYbatSokXr06CFJ2r9/v6ZNm6b09HR16NBBmzZt0tSpU5WZmanY2NiadgsAAAAALqvGp/C1b99eQ4YM8Zp1+jG73S6n02n+CQ4ONrcdPXpU27dv16hRo9S6dWu1adNGw4YNU3Z2tk6cOCFJ2rRpk8rLyzV69Gi1aNFCnTt31n333afly5eb+1m5cqVSUlLUt29fxcTEaMiQIWrZsqVWr15d0y4BAAAAgCU1noGyYu/evRo+fLgaNWqkdu3aaciQIWrcuLEkKScnR40aNdKNN95otk9KSpLNZtOBAwfUsWNH5eTkqG3btl4zV8nJyVq6dKlOnjyp4OBg5eTkqE+fPl7HTU5O1ueff37RusrKylRWVmYu22w2BQUFmT+j/qn8vfP7B1DfMQ4CYBywptYDVEpKiu644w5FRESooKBACxYs0F//+ldNmTJFfn5+8ng8CgkJ8XqNv7+/goOD5fF4JEkej0cRERFebZxOp7mtsm1oaKhXm9DQUHMf1Vm8eLEWLVpkLickJCgzM1PNmjW78g7jZyEyMtLXJQA+lefrAuBzUVFRvi4BPsY4AMYBa2o9QHXu3Nn8OTY2VnFxcRo7dqz27NmjpKSk2j5cjQwYMMBr1qoyZbvdbpWXl/uqLPiQzWZTZGSkCgoKZBiGr8sBAJ/Jz8/3dQkAfKy+jwN2u93SxEqdnMJ3oebNm6tx48YqKChQUlKSnE6nvv32W68258+f18mTJ81ZJqfTWWUmqXL5wjYlJSVebUpKSszt1XE4HHI4HNVu48tz/WYYBp8BAPUaYyAAxgFr6vw5UP/973918uRJNWnSRJKUmJioU6dOKTc312yze/duGYahVq1amW327dvnNSu0c+dORUdHmzekSExM1K5du7yOtXPnTrVu3bquuwQAAACgnqpxgDpz5oxcLpdcLpckqbCwUC6XS0VFRTpz5oz++c9/KicnR4WFhdq1a5deeuklRUZGKjk5WZIUExOjlJQUvfnmmzpw4IC++uorzZ49W506dVLTpk0lSV26dJHdbteMGTOUl5en7OxsrVq1yuv0u/vvv187duzQBx98oGPHjundd9/VwYMHlZaWVgtvCwAAAABUZTNqOFe3Z88eTZ48ucr61NRUjRgxQlOnTtWhQ4d06tQpNW3aVLfeeqsGDx7sdWrdyZMnlZWV5fUg3WHDhl30QbqNGzdWWlqa+vfv73XMzZs3a+HChXK73YqKirriB+m63W6vu/Oh/rDZbIqKilJ+fj7T1qjXzo/o6+sS4GP+M5f5ugT4GOMA6vs44HA4LF0DVeMA9XNEgKq/CFDA9/jihPr+xQmMA2AcsBqg6vwaKAAAAAD4uSBAAQAAAIBFBCgAAAAAsIgABQAAAAAWEaAAAAAAwCICFAAAAABYRIACAAAAAIsIUAAAAABgEQEKAAAAACwiQAEAAACARQQoAAAAALCIAAUAAAAAFhGgAAAAAMAiAhQAAAAAWESAAgAAAACLCFAAAAAAYBEBCgAAAAAsIkABAAAAgEUEKAAAAACwiAAFAAAAABYRoAAAAADAIgIUAAAAAFhEgAIAAAAAiwhQAAAAAGARAQoAAAAALCJAAQAAAIBFBCgAAAAAsIgABQAAAAAWEaAAAAAAwCICFAAAAABYRIACAAAAAIsIUAAAAABgEQEKAAAAACyy1/QFe/fu1bJly3To0CEVFxfr6aefVseOHSVJ5eXlWrhwobZt26bCwkI1bNhQSUlJSk9PV9OmTc19jBkzRm6322u/6enp6t+/v7l8+PBhZWVl6eDBgwoJCVFaWpr69evn9ZrNmzfrnXfekdvtVmRkpB5++GF16NChpl0CAAAAAEtqHKDOnj2r+Ph4de/eXX/729+8tp07d06HDh3SwIEDFR8fr5MnT2ru3Ll66aWX9OKLL3q1HTRokHr06GEuBwYGmj+XlpYqIyNDSUlJGjFihI4cOaI33nhDjRo1Ml+zf/9+TZs2Tenp6erQoYM2bdqkqVOnKjMzU7GxsTXtFgAAAABcVo0DVPv27dW+fftqtzVs2FATJkzwWjds2DA9++yzKioqUnh4uLk+KChITqez2v1s2rRJ5eXlGj16tOx2u1q0aCGXy6Xly5ebAWrlypVKSUlR3759JUlDhgzRrl27tHr1ao0cObKm3QIAAACAy6pxgKqp0tJS2Ww2NWzY0Gv9kiVL9P777ys8PFxdunRR79695e/vL0nKyclR27ZtZbf/UF5ycrKWLl2qkydPKjg4WDk5OerTp4/XPpOTk/X5559ftJaysjKVlZWZyzabTUFBQebPqH8qf+/8/gHUd4yDABgHrKnTAHXu3Dm9/fbb6ty5s1eAuu+++5SQkKDg4GDt379fCxYsUHFxsYYOHSpJ8ng8ioiI8NpX5WyVx+NRcHCwPB6PQkNDvdqEhobK4/FctJ7Fixdr0aJF5nJCQoIyMzPVrFmzn9hTXO8iIyN9XQLgU3m+LgA+FxUV5esS4GOMA2AcsKbOAlR5ebn+93//V5I0fPhwr20XzhzFxcXJbrdr5syZSk9Pl8PhqKuSNGDAAK9jV6Zst9ut8vLyOjsurl02m02RkZEqKCiQYRi+LgcAfCY/P9/XJQDwsfo+DtjtdksTK3USoCrDU1FRkZ577rkqp+/9WOvWrXX+/Hm53W5FR0fL6XRWmUmqXK6ciXI6nSopKfFqU1JSctHrqiTJ4XBcNKDx5bl+MwyDzwCAeo0xEADjgDW1/hyoyvBUUFCgCRMmqHHjxpd9jcvlks1mU0hIiCQpMTFR+/bt85oV2rlzp6KjoxUcHGy22bVrl9d+du7cqdatW9dibwAAAADgBzUOUGfOnJHL5ZLL5ZIkFRYWyuVyqaioSOXl5fr73/+u3NxcjR07VhUVFfJ4PPJ4PGYYysnJ0YoVK+RyufTNN99o48aNmjdvnrp27WqGoy5dushut2vGjBnKy8tTdna2Vq1a5XX63f33368dO3bogw8+0LFjx/Tuu+/q4MGDSktLq4W3BQAAAACqshk1nKvbs2ePJk+eXGV9amqqHnroIf32t7+t9nUTJ07ULbfcotzcXGVlZenYsWMqKytTRESE7rrrLvXp08fr9LoLH6TbuHFjpaWleT1oV/r+QboLFy6U2+1WVFTUFT9I1+12e92dD/WHzWZTVFSU8vPzmbZGvXZ+RF9flwAf85+5zNclwMcYB1DfxwGHw2HpGqgaB6ifIwJU/UWAAr7HFyfU9y9OYBwA44DVAFXr10ABAAAAwM8VAQoAAAAALCJAAQAAAIBFBCgAAAAAsIgABQAAAAAWEaAAAAAAwCICFAAAAABYRIACAAAAAIsIUAAAAABgEQEKAAAAACwiQAEAAACARQQoAAAAALCIAAUAAAAAFhGgAAAAAMAiAhQAAAAAWESAAgAAAACLCFAAAAAAYBEBCgAAAAAsIkABAAAAgEUEKAAAAACwiAAFAAAAABYRoAAAAADAIgIUAAAAAFhEgAIAAAAAiwhQAAAAAGARAQoAAAAALCJAAQAAAIBFBCgAAAAAsIgABQAAAAAWEaAAAAAAwCICFAAAAABYRIACAAAAAIsIUAAAAABgkb2mL9i7d6+WLVumQ4cOqbi4WE8//bQ6duxobjcMQ++++67WrVunU6dOqU2bNho+fLiioqLMNidPntTs2bO1detW2Ww23XHHHXr88ccVGBhotjl8+LCysrJ08OBBhYSEKC0tTf369fOqZfPmzXrnnXfkdrsVGRmphx9+WB06dLiS9wEAAAAALqvGM1Bnz55VfHy8nnjiiWq3L126VKtWrdKIESP017/+VQEBAZoyZYrOnTtntnn11VeVl5en8ePHa9y4cdq3b5/efPNNc3tpaakyMjIUHh6uF198UY888ojee+89ffTRR2ab/fv3a9q0aerevbsyMzN1++23a+rUqTpy5EhNuwQAAAAAltQ4QLVv315DhgzxmnWqZBiGVq5cqQceeEC333674uLi9Nvf/lbFxcX6/PPPJUlHjx7V9u3bNWrUKLVu3Vpt2rTRsGHDlJ2drRMnTkiSNm3apPLyco0ePVotWrRQ586ddd9992n58uXmsVauXKmUlBT17dtXMTExGjJkiFq2bKnVq1df6XsBAAAAAJdU41P4LqWwsFAej0e33nqrua5hw4Zq1aqVcnJy1LlzZ+Xk5KhRo0a68cYbzTZJSUmy2Ww6cOCAOnbsqJycHLVt21Z2+w/lJScna+nSpTp58qSCg4OVk5OjPn36eB0/OTnZDGrVKSsrU1lZmblss9kUFBRk/oz6p/L3zu8fQH3HOAiAccCaWg1QHo9HkhQaGuq1PjQ01Nzm8XgUEhLitd3f31/BwcFebSIiIrzaOJ1Oc1tl20sdpzqLFy/WokWLzOWEhARlZmaqWbNmFnuIn6vIyEhflwD4VJ6vC4DPXXitMuonxgEwDlhTqwHqWjdgwACvWavKlO12u1VeXu6rsuBDNptNkZGRKigokGEYvi4HAHwmPz/f1yUA8LH6Pg7Y7XZLEyu1GqAqZ4lKSkrUpEkTc31JSYni4+PNNt9++63X686fP6+TJ0+ar3c6nVVmkiqXL2xTUlLi1aakpMTcXh2HwyGHw1HtNr4812+GYfAZAFCvMQYCYBywplafAxURESGn06ldu3aZ60pLS3XgwAElJiZKkhITE3Xq1Cnl5uaabXbv3i3DMNSqVSuzzb59+7xmhXbu3Kno6GgFBwebbS48TmWb1q1b12aXAAAAAMBU4wB15swZuVwuuVwuSd/fOMLlcqmoqEg2m03333+//vWvf+mLL77QkSNHNH36dDVp0kS33367JCkmJkYpKSl68803deDAAX311VeaPXu2OnXqpKZNm0qSunTpIrvdrhkzZigvL0/Z2dlatWqV1+l3999/v3bs2KEPPvhAx44d07vvvquDBw8qLS2tFt4WAAAAAKjKZtRwrm7Pnj2aPHlylfWpqakaM2aM+SDdjz76SKWlpWrTpo2eeOIJRUdHm21PnjyprKwsrwfpDhs27KIP0m3cuLHS0tLUv39/r2Nu3rxZCxculNvtVlRU1BU/SNftdnvdnQ/1h81mU1RUlPLz85m2Rr12fkRfX5cAH/OfuczXJcDHGAdQ38cBh8Nh6RqoGgeonyMCVP1FgAK+xxcn1PcvTmAcAOOA1QBVq9dAAQAAAMDPGQEKAAAAACwiQAEAAACARQQoAAAAALCIAAUAAAAAFhGgAAAAAMAiAhQAAAAAWESAAgAAAACLCFAAAAAAYBEBCgAAAAAsIkABAAAAgEUEKAAAAACwiAAFAAAAABYRoAAAAADAIgIUAAAAAFhEgAIAAAAAiwhQAAAAAGARAQoAAAAALCJAAQAAAIBFBCgAAAAAsIgABQAAAAAWEaAAAAAAwCICFAAAAABYRIACAAAAAIsIUAAAAABgEQEKAAAAACwiQAEAAACARQQoAAAAALCIAAUAAAAAFhGgAAAAAMAiAhQAAAAAWESAAgAAAACLCFAAAAAAYBEBCgAAAAAsstf2DseMGSO3211lfc+ePTV8+HBNmjRJe/fu9drWo0cPjRw50lwuKirSzJkztWfPHgUGBio1NVXp6eny9/c32+zZs0fz589XXl6ewsLCNHDgQHXr1q22uwMAAAAAploPUC+88IIqKirM5SNHjigjI0N33nmnue6ee+7R4MGDzeUGDRqYP1dUVOiFF16Q0+lURkaGiouLNX36dPn7+ys9PV2SVFhYqBdffFH33nuvxo4dq927d2vGjBlyOp1KSUmp7S4BAAAAgKQ6CFAhISFey0uWLFHz5s118803m+sCAgLkdDqrff2OHTt09OhRTZgwQU6nU/Hx8Ro8eLDefvttDRo0SHa7XWvXrlVERIQeffRRSVJMTIy++uorrVixggAFAAAAoM7UeoC6UHl5uTZu3KjevXvLZrOZ6zdu3KiNGzfK6XTqF7/4hQYOHKiAgABJUk5OjmJjY70CVkpKimbNmqW8vDwlJCTo66+/VlJSktexkpOTNXfu3EvWU1ZWprKyMnPZZrMpKCjI/Bn1T+Xvnd8/gPqOcRAA44A1dRqgtmzZolOnTnldm9SlSxeFh4eradOmOnz4sN5++20dP35cTz/9tCTJ4/FUmZ0KDQ01t1X+XbnuwjanT5/WuXPnvE4JvNDixYu1aNEiczkhIUGZmZlq1qzZT+wprneRkZG+LgHwqTxfFwCfi4qK8nUJ8DHGATAOWFOnAWr9+vVKSUlR06ZNzXU9evQwf46NjVWTJk30/PPPq6CgoM6/xA4YMEB9+vQxlytTttvtVnl5eZ0eG9cmm82myMhIFRQUyDAMX5cDAD6Tn5/v6xIA+Fh9HwfsdruliZU6C1But1s7d+40Z5YuplWrVpJkBiin06kDBw54tSkpKZEkc2bK6XSa6y5sExQUdNHZJ0lyOBxyOBzVbuPLc/1mGAafAQD1GmMgAMYBa+rsOVDr169XaGioOnTocMl2LpdLktSkSRNJUmJioo4cOeIVkHbu3KmgoCDFxMRIklq3bq1du3Z57Wfnzp1KTEysxR4AAAAAgLc6CVAVFRXasGGDUlNTvZ7dVFBQoEWLFik3N1eFhYX64osv9Nprr6lt27aKi4uT9P3NIGJiYjR9+nS5XC5t375dCxcuVK9evczZo549e6qwsFBvvfWWjh07pjVr1mjz5s3q3bt3XXQHAAAAACTV0Sl8u3btUlFRke6++27vg9nt2rVrl1auXKmzZ88qLCxMd9xxhx544AGzjZ+fn8aNG6dZs2Zp/PjxCggIUGpqqtdzoyIiIjRu3DjNmzdPK1euVFhYmEaNGsUtzAEAAADUKZvByY5yu91etzdH/WGz2RQVFaX8/HzO+0W9dn5EX1+XAB/zn7nM1yXAxxgHUN/HAYfD4dubSOD6wGDJbVvr+2AJAABQE3V2EwkAAAAA+LkhQAEAAACARQQoAAAAALCIAAUAAAAAFhGgAAAAAMAiAhQAAAAAWESAAgAAAACLCFAAAAAAYBEBCgAAAAAsIkABAAAAgEUEKAAAAACwiAAFAAAAABYRoAAAAADAIgIUAAAAAFhEgAIAAAAAiwhQAAAAAGARAQoAAAAALCJAAQAAAIBFBCgAAAAAsIgABQAAAAAWEaAAAAAAwCICFAAAAABYRIACAAAAAIsIUAAAAABgEQEKAAAAACwiQAEAAACARQQoAAAAALCIAAUAAAAAFhGgAAAAAMAiAhQAAAAAWESAAgAAAACLCFAAAAAAYJG9tnf47rvvatGiRV7roqOj9corr0iSzp07p/nz5ys7O1tlZWVKTk7W8OHD5XQ6zfZFRUWaOXOm9uzZo8DAQKWmpio9PV3+/v5mmz179mj+/PnKy8tTWFiYBg4cqG7dutV2dwAAAADAVOsBSpJatGihCRMmmMt+fj9MdM2bN09ffvmlnnrqKTVs2FBZWVl6+eWX9Ze//EWSVFFRoRdeeEFOp1MZGRkqLi7W9OnT5e/vr/T0dElSYWGhXnzxRd17770aO3asdu/erRkzZsjpdColJaUuugQAAAAAdXMKn5+fn5xOp/knJCREklRaWqqPP/5YQ4cOVbt27dSyZUuNHj1a+/fvV05OjiRpx44dOnr0qMaOHav4+Hi1b99egwcP1po1a1ReXi5JWrt2rSIiIvToo48qJiZGaWlp+uUvf6kVK1bURXcAAAAAQFIdzUAVFBTof/7nf+RwOJSYmKj09HSFh4crNzdX58+fV1JSktn2hhtuUHh4uHJycpSYmKicnBzFxsZ6ndKXkpKiWbNmKS8vTwkJCfr666+99iFJycnJmjt37iXrKisrU1lZmblss9kUFBRk/gzUR3z2AUiMBQAYB6yq9QDVunVrjR49WtHR0SouLtaiRYv03HPP6eWXX5bH45HdblejRo28XhMaGiqPxyNJ8ng8XuGpcnvltsq/K9dd2Ob06dM6d+6cGjRoUG1tixcv9ro+KyEhQZmZmWrWrNlP6PH1Lc/XBcDnoqKifF0CrgGMBWAsAOMAGAesqfUA1b59e/PnuLg4M1Bt3rz5osHmahkwYID69OljLlembLfbbZ4eCNQ3+fn5vi4BwDWAsQBAfR8H7Ha7pYmVOjmF70KNGjVSdHS0CgoKdOutt6q8vFynTp3ymoUqKSkxZ52cTqcOHDjgtY+SkhJzW+XflesubBMUFHTJkOZwOORwOKrdZhhGTbsG/Czw2QcgMRYAYBywqs6fA3XmzBkVFBTI6XSqZcuW8vf3165du8ztx48fV1FRkRITEyVJiYmJOnLkiFdA2rlzp4KCghQTEyPp+9MEL9xHZZvKfQAAAABAXaj1ADV//nzt3btXhYWF2r9/v6ZOnSo/Pz916dJFDRs2VPfu3TV//nzt3r1bubm5ev3115WYmGiGn+TkZMXExGj69OlyuVzavn27Fi5cqF69epmzRz179lRhYaHeeustHTt2TGvWrNHmzZvVu3fv2u4OAAAAAJhq/RS+EydOaNq0afruu+8UEhKiNm3aaMqUKeatzIcOHSqbzaaXX35Z5eXl5oN0K/n5+WncuHGaNWuWxo8fr4CAAKWmpmrw4MFmm4iICI0bN07z5s3TypUrFRYWplGjRvEMKAAAAAB1ymZwsqPcbrfX7c3rk/Mj+vq6BPiY/8xlvi4B1wDGAjAWgHEA9X0ccDgclm4iUefXQAEAAADAzwUBCgAAAAAsIkABAAAAgEUEKAAAAACwiAAFAAAAABYRoAAAAADAIgIUAAAAAFhEgAIAAAAAiwhQAAAAAGARAQoAAAAALCJAAQAAAIBFBCgAAAAAsIgABQAAAAAWEaAAAAAAwCICFAAAAABYRIACAAAAAIsIUAAAAABgEQEKAAAAACwiQAEAAACARQQoAAAAALCIAAUAAAAAFhGgAAAAAMAiAhQAAAAAWESAAgAAAACLCFAAAAAAYBEBCgAAAAAsIkABAAAAgEUEKAAAAACwiAAFAAAAABYRoAAAAADAIgIUAAAAAFhEgAIAAAAAiwhQAAAAAGCRvbZ3uHjxYm3ZskXHjh1TgwYNlJiYqEceeUTR0dFmm0mTJmnv3r1er+vRo4dGjhxpLhcVFWnmzJnas2ePAgMDlZqaqvT0dPn7+5tt9uzZo/nz5ysvL09hYWEaOHCgunXrVttdAgAAAABJdRCg9u7dq169eunGG2/U+fPntWDBAmVkZOjvf/+7AgMDzXb33HOPBg8ebC43aNDA/LmiokIvvPCCnE6nMjIyVFxcrOnTp8vf31/p6emSpMLCQr344ou69957NXbsWO3evVszZsyQ0+lUSkpKbXcLAAAAAGr/FL4///nP6tatm1q0aKH4+HiNGTNGRUVFys3N9WoXEBAgp9Np/mnYsKG5bceOHTp69KjGjh2r+Ph4tW/fXoMHD9aaNWtUXl4uSVq7dq0iIiL06KOPKiYmRmlpafrlL3+pFStW1HaXAAAAAEDSVbgGqrS0VJIUHBzstX7jxo164okn9Ic//EH/93//p7Nnz5rbcnJyFBsbK6fTaa5LSUnR6dOnlZeXJ0n6+uuvlZSU5LXP5ORk5eTk1FFPAAAAANR3tX4K34UqKio0d+5c3XTTTYqNjTXXd+nSReHh4WratKkOHz6st99+W8ePH9fTTz8tSfJ4PF7hSZJCQ0PNbZV/V667sM3p06d17tw5r1MCK5WVlamsrMxcttlsCgoKMn8G6iM++wAkxgIAjANW1WmAysrKUl5enp5//nmv9T169DB/jo2NVZMmTfT888+roKBAkZGRdVbP4sWLtWjRInM5ISFBmZmZatasWZ0d81qX5+sC4HNRUVG+LgHXAMYCMBaAcQCMA9bUWYDKysrSl19+qcmTJyssLOySbVu1aiVJZoByOp06cOCAV5uSkhJJMmemnE6nue7CNkFBQdXOPknSgAED1KdPH3O5MmW73W7z2iqgvsnPz/d1CQCuAYwFAOr7OGC32y1NrNR6gDIMQ7Nnz9aWLVs0adIkRUREXPY1LpdLktSkSRNJUmJiov71r3+ppKTEPE1v586dCgoKUkxMjCSpdevW2rZtm9d+du7cqcTExIsex+FwyOFwXLRuoD7isw9AYiwAwDhgVa3fRCIrK0sbN27Uk08+qaCgIHk8Hnk8Hp07d07S97NMixYtUm5urgoLC/XFF1/otddeU9u2bRUXFyfp+5tBxMTEaPr06XK5XNq+fbsWLlyoXr16mQGoZ8+eKiws1FtvvaVjx45pzZo12rx5s3r37l3bXQIAAAAASXUwA7V27VpJ3z8s90KjR49Wt27dZLfbtWvXLq1cuVJnz55VWFiY7rjjDj3wwANmWz8/P40bN06zZs3S+PHjFRAQoNTUVK/nRkVERGjcuHGaN2+eVq5cqbCwMI0aNYpnQAEAAACoMzaDuTq53W6vu/PVJ+dH9PV1CfAx/5nLfF0CrgGMBWAsAOMA6vs44HA4LF0DVefPgQIAAACAnwsCFAAAAABYRIACAAAAAIsIUAAAAABgEQEKAAAAACwiQAEAAACARQQoAAAAALCIAAUAAAAAFhGgAAAAAMAiAhQAAAAAWESAAgAAAACLCFAAAAAAYBEBCgAAAAAsIkABAAAAgEUEKAAAAACwiAAFAAAAABYRoAAAAADAIgIUAAAAAFhEgAIAAAAAiwhQAAAAAGARAQoAAAAALCJAAQAAAIBFBCgAAAAAsIgABQAAAAAWEaAAAAAAwCICFAAAAABYRIACAAAAAIsIUAAAAABgEQEKAAAAACwiQAEAAACARQQoAAAAALCIAAUAAAAAFhGgAAAAAMAiAhQAAAAAWGT3dQE/1erVq/XBBx/I4/EoLi5Ow4YNU6tWrXxdFgAAAICfoet6Bio7O1vz58/Xgw8+qMzMTMXFxWnKlCkqKSnxdWkAAAAAfoau6wC1fPly3XPPPbr77rsVExOjESNGqEGDBlq/fr2vSwMAAADwM3TdnsJXXl6u3Nxc9e/f31zn5+enpKQk5eTkVPuasrIylZWVmcs2m01BQUGy26/bt+En87vxJl+XAB/zdzh8XQKuAYwFYCwA4wDq+zhgNRNct8nh22+/VUVFhZxOp9d6p9Op48ePV/uaxYsXa9GiReZy586d9eSTT6pJkyZ1Weq17dW3fV0BgGsBYwEAxgHAkuv6FL6aGjBggObOnWv+GTFihNeMFOqf06dP609/+pNOnz7t61IA+AjjAADGAdTEdTsDFRISIj8/P3k8Hq/1Ho+nyqxUJYfDIUc9n5qEN8MwdOjQIRmG4etSAPgI4wAAxgHUxHU7A2W329WyZUvt3r3bXFdRUaHdu3crMTHRh5UBAAAA+Lm6bmegJKlPnz567bXX1LJlS7Vq1UorV67U2bNn1a1bN1+XBgAAAOBn6LoOUJ06ddK3336rd999Vx6PR/Hx8Xr22Wcvegof8GMOh0MPPvggp3YC9RjjAADGAdSEzeBkTwAAAACw5Lq9BgoAAAAArjYCFAAAAABYRIACAAAAAIsIUAAAAABgEQEKAAAAACy6rm9jDtTUt99+q/Xr1ysnJ0cej0eS5HQ6ddNNN6lbt24KCQnxbYEAAAC4pjEDhXrjwIEDevLJJ7Vq1So1bNhQbdu2Vdu2bdWwYUOtWrVKv//973Xw4EFflwnAx4qKivT666/7ugwAdejcuXP66quvdPTo0Wq3ffLJJz6oCtcLZqBQb8yZM0d33nmnRowYIZvN5rXNMAzNnDlTs2fP1pQpU3xUIYBrwcmTJ/XJJ59o9OjRvi4FQB04fvy4pkyZoqKiIklSmzZt9Pvf/15NmjSRJJWWlur1119XamqqL8vENYwAhXrD5XJp9OjRVcKTJNlsNvXu3VvPPPOMDyoDcDV98cUXl9z+zTffXKVKAPjC22+/rRYtWuiFF15QaWmp5s6dqwkTJmjSpEkKDw/3dXm4DhCgUG84nU4dOHBAN9xwQ7XbDxw4IKfTeXWLAnDVTZ061dclAPChnJwcTZgwQSEhIQoJCdGf/vQnzZo1S88995wmTpyogIAAX5eIaxwBCvXGr371K/3jH/9Qbm6ukpKSFBoaKkkqKSnRrl27tG7dOv3mN7/xcZUA6prT6dTw4cN1++23V7vd5XLpT3/601WuCsDVcu7cOfn5/XAbAJvNphEjRigrK0uTJk3S7373Ox9Wh+sBAQr1RlpamkJCQrRixQqtXbtWFRUVkiQ/Pz+1bNlSo0ePVqdOnXxcJYC61rJlS+Xm5l40QAH4eYuOjlZubq5iYmK81j/xxBOSpJdeeskXZeE6YjMMw/B1EcDVVl5eru+++06S1LhxY9nt/L8EoL7Yt2+fzp49q5SUlGq3nzlzRrm5ubr55puvbmEArorFixfrq6++0v/7f/+v2u2zZs3Shx9+qHfeeecqV4brBQEKAAAAACziOVAAAAAAYBEBCgAAAAAsIkABAAAAgEUEKAAAAACwiAAFAPXchg0bNGjQIB08eLDa7ZMmTdIf/vCHK9r3mjVrtGHDhp9QnbfCwkI98sgjeuWVV6rdnp2drUGDBmn16tW1dsyrrfL38fDDD+vEiRNVtv+U3wcA4KcjQAEA6szatWtrNUBFRETowQcfVHZ2tnbs2OG1rbS0VPPmzVPr1q3Vs2fPWjumr5SVlWnJkiW+LgMA8CMEKADAdeVXv/qVYmNjNWvWLJ07d85cv3DhQn377bcaOXKk/Pyu7X/ezpw5c9k28fHxWrduXbWzUAAA3+HpoQCAGlu/fr0+/fRT5eXlqbS0VM2bN9d9993nNfMzZswYud1uSdKgQYMkSTfffLMmTZokSTp16pTee+89/ec//1FJSYnCwsJ0zz33qG/fvpcMQP7+/ho5cqQmTJig999/X7/+9a+Vm5urNWvWqG/fvoqLi7NU3/Tp07Vt2za9+eabVR6mnZGRIbfbrWnTpl3yfdi8ebOWLFmio0ePKjAwUMnJyXrkkUfUtGlTs81rr72mzz77TFOnTtWcOXO0b98+tWvXTs8888wl9z1gwAC9+uqrWrJkiYYNG3bJtlervwAAAhQA4P9XWlqqb7/9tsr68+fPV1m3du1atWjRQrfddpv8/f21detWzZo1SxUVFUpLS5MkDR06VHPmzFFgYKAGDBggSXI6nZKks2fPatKkSTpx4oR69Oih8PBw7d+/XwsWLJDH49Fjjz12yVoTExPVs2dPLVu2TJ07d9Y//vEPRURE6KGHHrJc31133aVPP/1UO3bs0C9+8Qtz3x6PR7t379aDDz54yRo2bNig119/XTfeeKPS09NVUlKilStXav/+/XrppZfUqFEjs21FRYWmTJmiNm3a6De/+Y0CAgIuuW/p+9MV77rrLq1bt079+/f3CmU/djX6CwD4HgEKACBJ+stf/nLRbS1atPBanjx5sho0aGAup6WlacqUKVqxYoX5hb1jx45655131LhxY911111er1++fLkKCgr00ksvKSoqSpJ07733qmnTplq2bJn69Omj8PDwS9b761//Wlu2bNGkSZN08uRJPfvss2ZNVupr166dwsLCtHHjRq9AsWnTJhmGUaXmC5WXl+vtt99WixYtvI7Vpk0bvfjii1qxYoU56yZ9fz3TnXfeqfT09Ev26cceeOABffrpp1q6dKkef/zxi7ar6/4CAH5AgAIASJKeeOIJM8xc6J///KcqKiq81l34Zb20tFTl5eW6+eabtWPHDpWWlqphw4aXPNZnn32mtm3bqlGjRl6zXklJSVqyZIn27dunrl27XnIfDRs21GOPPab//d//VadOnZSSklKj+vz8/NSlSxetWrVKp0+fVlBQkKTvA0ViYqIiIiIueuzc3FyVlJTooYce8jpWhw4ddMMNN+jLL7/0ClCSrujGFs2bN1fXrl310UcfqX///mrSpEm17eq6vwCAHxCgAACSpFatWunGG2+ssr5Ro0b67rvvvNZ99dVXeu+995STk6OzZ896bbMSoPLz83X48GENHz682u0lJSWWaq6st2XLlldUX2pqqpYuXaotW7YoNTVVx48fV25urkaMGHHJ41Ze2xUdHV1lW3R0tL766iuvdf7+/pc8Be9SBg4cqI0bN2rJkiUXnYWq6/4CAH5AgAIA1EhBQYH+8pe/KDo6Wo8++qjCwsJkt9u1bds2rVixospsVXUMw9Ctt96qvn37Vru9umBSF/XFxMSoZcuW2rhxo1JTU/Xpp5/KbrfrzjvvvOLjV8dut1/xnQF/PAv1Y9difwHg54wABQCoka1bt6qsrEx/+tOfvK5T2rNnj+V9NG/eXGfOnNGtt97q8/ruuusuzZ8/X8XFxfr3v/+tDh06KDg4+JLHaNasmSTp+PHjateunde248ePm9trywMPPKCNGzdq6dKlVbZdjf4CAH5wbT8oAwBwzamcSTEMw1xXWlpa7QNzAwMDderUqSrr77zzTuXk5Gj79u1Vtp06daraO//VRX2S1KVLF9lsNs2ZM0fffPPNZa+9kr4/ZTA0NFQffvihysrKzPXbtm3TsWPH1KFDhyuuvzqRkZHq2rWrPvzwQ3k8Hq9tV6O/AIAfMAMFAKiR5ORk2e12ZWZmqkePHjpz5ozWrVunkJAQFRcXe7VNSEjQhx9+qPfff1+RkZEKDQ1Vu3bt1LdvX33xxRfKzMxUamqqWrZsqbNnz+rIkSP67LPP9NprrykkJKTO65OkkJAQJScn67PPPlOjRo0shR+73a6HH35Yr7/+uiZNmqTOnTvL4/Fo1apVatasmXr37n1FtV9K5R35jh8/7nVXxKvRXwDAD5iBAgDUSHR0tJ566inZbDb985//1IcffqgePXro/vvvr9L2wQcfVPv27bVs2TJNmzZNixYtkiQFBARo8uTJ+tWvfqW9e/dq7ty5WrJkiQoKCjRo0KDL3oSituqrlJqaKun7mTGHw2HpON26ddPvf/9785bmH330kW6//Xb95S9/8XoGVG2pnIX6savVXwDA92zGhXP+AADUQ59//rmmTp2qyZMnq23btr4up87Vt/4CQG1iBgoAUO+tW7dOzZs3V5s2bXxdylVR3/oLALWJa6AAAPXWv//9bx0+fFhffvmlHnvsMdlsNl+XVKfqW38BoC4QoAAA9da0adMUGBio7t27q1evXr4up87Vt/4CQF3gGigAAAAAsIhroAAAAADAIgIUAAAAAFhEgAIAAAAAiwhQAAAAAGARAQoAAAAALCJAAQAAAIBFBCgAAAAAsIgABQAAAAAWEaAAAAAAwKL/D/PEKgudAg4vAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = df['label'].value_counts().sort_index() \\\n",
    "    .plot(kind='bar',\n",
    "          title='Count of hate or not',\n",
    "          figsize=(10, 5))\n",
    "ax.set_xlabel('Hate Yay or Nay')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to be heading out tomorrow to check out the Earth Festival in Davis #ImNotAHippie\n"
     ]
    }
   ],
   "source": [
    "example = df['text'][random.randint(0, 500)]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Going', 'to', 'be', 'heading', 'out', 'tomorrow', 'to', 'check', 'out', 'the']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(example)\n",
    "print(tokens[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Going', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('be', 'VB'),\n",
       " ('heading', 'VBG'),\n",
       " ('out', 'RB'),\n",
       " ('tomorrow', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('check', 'VB'),\n",
       " ('out', 'RP'),\n",
       " ('the', 'DT')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(tokens)\n",
    "tagged[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Going/VBG\n",
      "  to/TO\n",
      "  be/VB\n",
      "  heading/VBG\n",
      "  out/RB\n",
      "  tomorrow/NN\n",
      "  to/TO\n",
      "  check/VB\n",
      "  out/RP\n",
      "  the/DT\n",
      "  (ORGANIZATION Earth/NN Festival/NNP)\n",
      "  in/IN\n",
      "  (GPE Davis/NNP)\n",
      "  #/#\n",
      "  (ORGANIZATION ImNotAHippie/NNP))\n"
     ]
    }
   ],
   "source": [
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "entities.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "import contractions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hyperlinks_marks_styles(text):\n",
    "    new_text = re.sub(r'^RT[\\s]', '', text)\n",
    "    new_text = re.sub(r'@\\S+', '', new_text)\n",
    "    new_text = re.sub(r'https?:\\/\\/.*[\\r\\n]*','', new_text)\n",
    "    new_text = re.sub(r'#', '', new_text)\n",
    "\n",
    "    return new_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "\n",
    "    text_tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    return text_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\RajBu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "stopwords_english = stopwords.words('english')\n",
    "\n",
    "punctuations = string.punctuation\n",
    "\n",
    "def remove_punctuations_stopwords(text_tokens):\n",
    "\n",
    "    text_clean = []\n",
    "\n",
    "    for word in text_tokens:\n",
    "        if (word not in stopwords_english and word not in punctuations):\n",
    "            text_clean.append(word)\n",
    "\n",
    "    return text_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions_in_text(text):\n",
    "    return contractions.fix(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map NLTK POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ  # Adjective\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB  # Verb\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN  # Noun\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV  # Adverb\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text_tokens):\n",
    "    pos_tags = pos_tag(text_tokens)  # POS tagging\n",
    "    return [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = remove_hyperlinks_marks_styles(text)  # Remove unnecessary elements\n",
    "    text = expand_contractions_in_text(text)\n",
    "    tokens = tokenize_text(text)  # Tokenize\n",
    "    tokens = remove_punctuations_stopwords(tokens)  # Remove stopwords & punctuation\n",
    "    tokens = lemmatize_text(tokens)  # Lemmatize with POS tagging\n",
    "    return \" \".join(tokens)  # Convert back to string if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Who wants to be my date to the White Sox vs Red Sox game Tuesday\n",
      "Processed: Who want date White Sox v Red Sox game Tuesday\n"
     ]
    }
   ],
   "source": [
    "text = df['text'][random.randint(0, 500)]\n",
    "processed_text = preprocess_text(text)\n",
    "\n",
    "print(\"Original:\", text)\n",
    "print(\"Processed:\", processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('sentiment_train.csv')\n",
    "val_data = pd.read_csv('sentiment_validation.csv')\n",
    "test_data = pd.read_csv('sentiment_test.csv')\n",
    "\n",
    "\n",
    "X_train, y_train = train_data['text'], train_data['label']\n",
    "X_val, y_val = val_data['text'], val_data['label']\n",
    "X_test, y_test = test_data['text'], test_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the text column is a string and handle NaNs\n",
    "X_train = X_train.astype(str).fillna('')\n",
    "X_val = X_val.astype(str).fillna('')\n",
    "X_test = X_test.astype(str).fillna('')\n",
    "\n",
    "X_train = X_train.apply(preprocess_text)\n",
    "X_val = X_val.apply(preprocess_text)\n",
    "X_test = X_test.apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        `` QT In original draft 7th book Remus Lupin s...\n",
      "1        `` Ben Smith Smith concussion remain lineup Th...\n",
      "2        Sorry bout stream last night I crash tonight s...\n",
      "3        Chase Headley 's RBI double 8th inning David P...\n",
      "4        Alciato Bee invest 150 million January another...\n",
      "                               ...                        \n",
      "45610    '' '' So amazing beautiful Lady Gaga show AC t...\n",
      "45611    9 September arrive mean Apple 's new iPhone ho...\n",
      "45612    Leeds 1-1 Sheff Wed. Giuseppe Bellusci secure ...\n",
      "45613    I hilton head till 8th lol go Jason aldean sep...\n",
      "45614    WASHINGTON Reuters YOU.S Vice President Joe Bi...\n",
      "Name: text, Length: 45615, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting text into numerical features using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=2, max_df=0.8) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on training data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform validation and test data\n",
    "X_val_tfidf = vectorizer.transform(X_val)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_recall_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converting labels to negative or non negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_binary = y_train.apply(lambda x: 1 if x == 0 else 0)  # 0 = negative (target), 1 & 2 = non-negative\n",
    "y_val_binary = y_val.apply(lambda x: 1 if x == 0 else 0)\n",
    "y_test_binary = y_test.apply(lambda x: 1 if x == 0 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 0.5, 1.0, 1.5,2.0, 5.0, 7.5, 10],  # Smoothing parameter\n",
    "    'fit_prior': [True, False]      # Whether to learn class prior probabilities\n",
    "}\n",
    "\n",
    "# Track the best model and score\n",
    "best_score = 0\n",
    "best_params = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Accuracy: 0.8565\n",
      "Best Hyperparameters: {'alpha': 0.1, 'fit_prior': True}\n"
     ]
    }
   ],
   "source": [
    "for alpha in param_grid['alpha']:\n",
    "    for fit_prior in param_grid['fit_prior']:\n",
    "        # Set model parameters\n",
    "        model.set_params(alpha=alpha, fit_prior=fit_prior)\n",
    "        \n",
    "        # Train on the training set\n",
    "        model.fit(X_train_tfidf, y_train_binary)\n",
    "        \n",
    "        # Evaluate on the validation set\n",
    "        val_predictions = model.predict(X_val_tfidf)\n",
    "        val_score = accuracy_score(y_val_binary, val_predictions)\n",
    "        \n",
    "        # Update best model if needed\n",
    "        if val_score > best_score:\n",
    "            best_score = val_score\n",
    "            best_params = {'alpha': alpha, 'fit_prior': fit_prior}\n",
    "\n",
    "print(f\"Best Validation Accuracy: {best_score}\")\n",
    "print(f\"Best Hyperparameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6769781830022794\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.99      0.81      8312\n",
      "           1       0.51      0.03      0.06      3972\n",
      "\n",
      "    accuracy                           0.68     12284\n",
      "   macro avg       0.59      0.51      0.43     12284\n",
      "weighted avg       0.62      0.68      0.56     12284\n",
      "\n",
      "Confusion Matrix:\n",
      " [[8198  114]\n",
      " [3854  118]]\n",
      "F1 Score: 0.056137012369172214\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on Test Set\n",
    "y_test_pred = model.predict(X_test_tfidf)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test_binary, y_test_pred))\n",
    "print(\"Test Classification Report:\\n\", classification_report(y_test_binary, y_test_pred))\n",
    "\n",
    "cm = confusion_matrix(y_test_binary, y_test_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "\n",
    "f1 = f1_score(y_test_binary, y_test_pred)\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nn_preprocess_text(text, remove_stopwords=True):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove hyperlinks etc.\n",
    "\n",
    "    text = remove_hyperlinks_marks_styles(text)\n",
    "\n",
    "    text = expand_contractions_in_text(text)\n",
    "\n",
    "    # Remove special characters, numbers, and punctuation\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "    \n",
    "    #Remove stopwords\n",
    "    if remove_stopwords:\n",
    "        text = ' '.join(word for word in text.split() if word not in stopwords_english)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: O. Johnson commits his 3rd foul! He\\u2019s cold off the bench with 3 fouls and a turnover in Q4. #Pacers losing 98-87\\u002c 4:52 left.\n",
      "Processed: johnson commits rd foul heus cold bench foul turnover q pacer losing uc left\n"
     ]
    }
   ],
   "source": [
    "text = df['text'][random.randint(0, 500)]\n",
    "processed_text = nn_preprocess_text(text)\n",
    "\n",
    "print(\"Original:\", text)\n",
    "print(\"Processed:\", processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_train_data = pd.read_csv('sentiment_train.csv')\n",
    "nn_val_data = pd.read_csv('sentiment_validation.csv')\n",
    "nn_test_data = pd.read_csv('sentiment_test.csv')\n",
    "\n",
    "\n",
    "nn_X_train, nn_y_train = nn_train_data['text'], nn_train_data['label']\n",
    "nn_X_val, nn_y_val = nn_val_data['text'], nn_val_data['label']\n",
    "nn_X_test, nn_y_test = nn_test_data['text'], nn_test_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the text column is a string and handle NaNs\n",
    "nn_X_train = nn_X_train.astype(str).fillna('')\n",
    "nn_X_val = nn_X_val.astype(str).fillna('')\n",
    "nn_X_test = nn_X_test.astype(str).fillna('')\n",
    "\n",
    "nn_X_train = nn_X_train.apply(nn_preprocess_text)\n",
    "nn_X_val = nn_X_val.apply(nn_preprocess_text)\n",
    "nn_X_test = nn_X_test.apply(nn_preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of 0        qt original draft th book remus lupin survived...\n",
      "1        ben smith smith concussion remains lineup thur...\n",
      "2        sorry bout stream last night crashed tonight s...\n",
      "3        chase headleys rbi double th inning david pric...\n",
      "4        alciato bee invest million january another sum...\n",
      "                               ...                        \n",
      "45610    amazing beautiful lady gaga show ac tonight lo...\n",
      "45611    september ha arrived mean apple new iphone hou...\n",
      "45612    leeds sheff wed giuseppe bellusci securing luf...\n",
      "45613    hilton head till th lol go jason aldean sept t...\n",
      "45614    washington reuters yous vice president joe bid...\n",
      "Name: text, Length: 45615, dtype: object>\n"
     ]
    }
   ],
   "source": [
    "print(nn_X_train.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(nn_X_train)\n",
    "\n",
    "nn_X_train_seq = tokenizer.texts_to_sequences(nn_X_train)\n",
    "nn_X_val_seq = tokenizer.texts_to_sequences(nn_X_val)\n",
    "nn_X_test_seq = tokenizer.texts_to_sequences(nn_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding sequences\n",
    "max_sequence_length = 100\n",
    "\n",
    "nn_X_train_padded = pad_sequences(nn_X_train_seq, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "nn_X_val_padded = pad_sequences(nn_X_val_seq, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "nn_X_test_padded = pad_sequences(nn_X_test_seq, maxlen=max_sequence_length, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converting labels to negative or non negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_y_train_binary = nn_y_train.apply(lambda x: 1 if x == 0 else 0)  # 0 = negative (target), 1 & 2 = non-negative\n",
    "nn_y_val_binary = nn_y_val.apply(lambda x: 1 if x == 0 else 0)\n",
    "nn_y_test_binary = nn_y_test.apply(lambda x: 1 if x == 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=100),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(32),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max'),\n",
    "    EarlyStopping(monitor='val_accuracy', patience=3, mode='max', verbose=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1426/1426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.8422 - loss: 0.4440"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1426/1426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 105ms/step - accuracy: 0.8422 - loss: 0.4440 - val_accuracy: 0.8440 - val_loss: 0.4333\n",
      "Epoch 2/10\n",
      "\u001b[1m1426/1426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 110ms/step - accuracy: 0.8454 - loss: 0.4346 - val_accuracy: 0.8440 - val_loss: 0.4334\n",
      "Epoch 3/10\n",
      "\u001b[1m1426/1426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 112ms/step - accuracy: 0.8423 - loss: 0.4396 - val_accuracy: 0.8440 - val_loss: 0.4331\n",
      "Epoch 4/10\n",
      "\u001b[1m1426/1426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 111ms/step - accuracy: 0.8465 - loss: 0.4314 - val_accuracy: 0.8440 - val_loss: 0.4339\n",
      "Epoch 4: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "history = model.fit(nn_X_train_padded, nn_y_train_binary, validation_data=(nn_X_val_padded, nn_y_val_binary), epochs=10, batch_size=32, callbacks=callbacks, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot accuracy per epoch\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy Per Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 44ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      1.00      0.81      8312\n",
      "           1       0.00      0.00      0.00      3972\n",
      "\n",
      "    accuracy                           0.68     12284\n",
      "   macro avg       0.34      0.50      0.40     12284\n",
      "weighted avg       0.46      0.68      0.55     12284\n",
      "\n",
      "[[8312    0]\n",
      " [3972    0]]\n",
      "Accuracy: 0.6766525561706285\n",
      "F1 Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RajBu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\RajBu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\RajBu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "nn_y_pred = (model.predict(nn_X_test_padded) > 0.5).astype('int32')\n",
    "print(classification_report(nn_y_test_binary, nn_y_pred))\n",
    "print(confusion_matrix(nn_y_test_binary, nn_y_pred))\n",
    "print('Accuracy:', accuracy_score(nn_y_test_binary, nn_y_pred))\n",
    "\n",
    "nn_f1 = f1_score(nn_y_test_binary, nn_y_pred)\n",
    "print(f\"F1 Score: {nn_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_train_data = pd.read_csv('sentiment_train.csv')\n",
    "bert_val_data = pd.read_csv('sentiment_validation.csv')\n",
    "bert_test_data = pd.read_csv('sentiment_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_train_data['label'] = bert_train_data['label'].apply(lambda x: 1 if x != 0 else 0)\n",
    "bert_val_data['label'] = bert_val_data['label'].apply(lambda x: 1 if x != 0 else 0)\n",
    "bert_test_data['label'] = bert_test_data['label'].apply(lambda x: 1 if x != 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"QT @user In the original draft of the 7th boo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Ben Smith / Smith (concussion) remains out of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sorry bout the stream last night I crashed out...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chase Headley's RBI double in the 8th inning o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@user Alciato: Bee will invest 150 million in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>@user I\\u2019m sick with something ill be at s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>\"There's something about Friday Night Lights, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Googled the snake I stepped over on the trail ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>@user Also, his anger against Hindus are justi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Ricky Ponting and I now have something in comm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  label\n",
       "0   \"QT @user In the original draft of the 7th boo...      1\n",
       "1   \"Ben Smith / Smith (concussion) remains out of...      1\n",
       "2   Sorry bout the stream last night I crashed out...      1\n",
       "3   Chase Headley's RBI double in the 8th inning o...      1\n",
       "4   @user Alciato: Bee will invest 150 million in ...      1\n",
       "..                                                ...    ...\n",
       "95  @user I\\u2019m sick with something ill be at s...      0\n",
       "96  \"There's something about Friday Night Lights, ...      1\n",
       "97  Googled the snake I stepped over on the trail ...      1\n",
       "98  @user Also, his anger against Hindus are justi...      0\n",
       "99  Ricky Ponting and I now have something in comm...      1\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_train_data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m bert_val_data \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(bert_val_data)\n\u001b[0;32m      3\u001b[0m bert_test_data \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(bert_test_data)\n\u001b[1;32m----> 4\u001b[0m short_bert_train_data \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbert_train_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\arrow_dataset.py:846\u001b[0m, in \u001b[0;36mDataset.from_pandas\u001b[1;34m(cls, df, features, info, split, preserve_index)\u001b[0m\n\u001b[0;32m    844\u001b[0m     info \u001b[38;5;241m=\u001b[39m DatasetInfo()\n\u001b[0;32m    845\u001b[0m info\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;241m=\u001b[39m features\n\u001b[1;32m--> 846\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[43mInMemoryTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreserve_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreserve_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    851\u001b[0m     \u001b[38;5;66;03m# more expensive cast than InMemoryTable.from_pandas(..., schema=features.arrow_schema)\u001b[39;00m\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# needed to support the str to Audio conversion for instance\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     table \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39mcast(features\u001b[38;5;241m.\u001b[39marrow_schema)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\table.py:719\u001b[0m, in \u001b[0;36mInMemoryTable.from_pandas\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pandas\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    665\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;124;03m    Convert pandas.DataFrame to an Arrow Table.\u001b[39;00m\n\u001b[0;32m    667\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    717\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyarrow\\table.pxi:4751\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_pandas\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyarrow\\pandas_compat.py:595\u001b[0m, in \u001b[0;36mdataframe_to_arrays\u001b[1;34m(df, schema, preserve_index, nthreads, columns, safe)\u001b[0m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdataframe_to_arrays\u001b[39m(df, schema, preserve_index, nthreads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    587\u001b[0m                         safe\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    588\u001b[0m     (all_names,\n\u001b[0;32m    589\u001b[0m      column_names,\n\u001b[0;32m    590\u001b[0m      column_field_names,\n\u001b[0;32m    591\u001b[0m      index_column_names,\n\u001b[0;32m    592\u001b[0m      index_descriptors,\n\u001b[0;32m    593\u001b[0m      index_columns,\n\u001b[0;32m    594\u001b[0m      columns_to_convert,\n\u001b[1;32m--> 595\u001b[0m      convert_fields) \u001b[38;5;241m=\u001b[39m \u001b[43m_get_columns_to_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;66;03m# NOTE(wesm): If nthreads=None, then we use a heuristic to decide whether\u001b[39;00m\n\u001b[0;32m    599\u001b[0m     \u001b[38;5;66;03m# using a thread pool is worth it. Currently the heuristic is whether the\u001b[39;00m\n\u001b[0;32m    600\u001b[0m     \u001b[38;5;66;03m# nrows > 100 * ncols and ncols > 1.\u001b[39;00m\n\u001b[0;32m    601\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nthreads \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyarrow\\pandas_compat.py:372\u001b[0m, in \u001b[0;36m_get_columns_to_convert\u001b[1;34m(df, schema, preserve_index, columns)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_columns_to_convert\u001b[39m(df, schema, preserve_index, columns):\n\u001b[1;32m--> 372\u001b[0m     columns \u001b[38;5;241m=\u001b[39m \u001b[43m_resolve_columns_of_interest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[0;32m    375\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    376\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDuplicate column names found: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlist\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[0;32m    377\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyarrow\\pandas_compat.py:546\u001b[0m, in \u001b[0;36m_resolve_columns_of_interest\u001b[1;34m(df, schema, columns)\u001b[0m\n\u001b[0;32m    544\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m columns \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 546\u001b[0m     columns \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m columns\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "bert_train_data = Dataset.from_pandas(bert_train_data)\n",
    "bert_val_data = Dataset.from_pandas(bert_val_data)\n",
    "bert_test_data = Dataset.from_pandas(bert_test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 45615/45615 [00:27<00:00, 1635.93 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:01<00:00, 1754.28 examples/s]\n",
      "Map: 100%|██████████| 12284/12284 [00:06<00:00, 1755.38 examples/s]\n"
     ]
    }
   ],
   "source": [
    "bert_train_data = bert_train_data.map(tokenize_function, batched=True)\n",
    "bert_val_data = bert_val_data.map(tokenize_function, batched=True)\n",
    "bert_test_data = bert_test_data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_train_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "bert_val_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "bert_test_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=1,              # training epochs\n",
    "    per_device_train_batch_size=16,   # batch size for training\n",
    "    per_device_eval_batch_size=16,    # batch size for evaluation\n",
    "    warmup_steps=500,                # warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m num_samples_to_take \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8600\u001b[39m\n\u001b[1;32m----> 2\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbert_test_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m ds \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mtake(num_samples_to_take)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\arrow_dataset.py:846\u001b[0m, in \u001b[0;36mDataset.from_pandas\u001b[1;34m(cls, df, features, info, split, preserve_index)\u001b[0m\n\u001b[0;32m    844\u001b[0m     info \u001b[38;5;241m=\u001b[39m DatasetInfo()\n\u001b[0;32m    845\u001b[0m info\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;241m=\u001b[39m features\n\u001b[1;32m--> 846\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[43mInMemoryTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreserve_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreserve_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    851\u001b[0m     \u001b[38;5;66;03m# more expensive cast than InMemoryTable.from_pandas(..., schema=features.arrow_schema)\u001b[39;00m\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# needed to support the str to Audio conversion for instance\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     table \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39mcast(features\u001b[38;5;241m.\u001b[39marrow_schema)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\table.py:719\u001b[0m, in \u001b[0;36mInMemoryTable.from_pandas\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pandas\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    665\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;124;03m    Convert pandas.DataFrame to an Arrow Table.\u001b[39;00m\n\u001b[0;32m    667\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    717\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyarrow\\table.pxi:4751\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_pandas\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyarrow\\pandas_compat.py:595\u001b[0m, in \u001b[0;36mdataframe_to_arrays\u001b[1;34m(df, schema, preserve_index, nthreads, columns, safe)\u001b[0m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdataframe_to_arrays\u001b[39m(df, schema, preserve_index, nthreads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    587\u001b[0m                         safe\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    588\u001b[0m     (all_names,\n\u001b[0;32m    589\u001b[0m      column_names,\n\u001b[0;32m    590\u001b[0m      column_field_names,\n\u001b[0;32m    591\u001b[0m      index_column_names,\n\u001b[0;32m    592\u001b[0m      index_descriptors,\n\u001b[0;32m    593\u001b[0m      index_columns,\n\u001b[0;32m    594\u001b[0m      columns_to_convert,\n\u001b[1;32m--> 595\u001b[0m      convert_fields) \u001b[38;5;241m=\u001b[39m \u001b[43m_get_columns_to_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;66;03m# NOTE(wesm): If nthreads=None, then we use a heuristic to decide whether\u001b[39;00m\n\u001b[0;32m    599\u001b[0m     \u001b[38;5;66;03m# using a thread pool is worth it. Currently the heuristic is whether the\u001b[39;00m\n\u001b[0;32m    600\u001b[0m     \u001b[38;5;66;03m# nrows > 100 * ncols and ncols > 1.\u001b[39;00m\n\u001b[0;32m    601\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nthreads \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyarrow\\pandas_compat.py:372\u001b[0m, in \u001b[0;36m_get_columns_to_convert\u001b[1;34m(df, schema, preserve_index, columns)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_columns_to_convert\u001b[39m(df, schema, preserve_index, columns):\n\u001b[1;32m--> 372\u001b[0m     columns \u001b[38;5;241m=\u001b[39m \u001b[43m_resolve_columns_of_interest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[0;32m    375\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    376\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDuplicate column names found: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlist\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[0;32m    377\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyarrow\\pandas_compat.py:546\u001b[0m, in \u001b[0;36m_resolve_columns_of_interest\u001b[1;34m(df, schema, columns)\u001b[0m\n\u001b[0;32m    544\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m columns \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 546\u001b[0m     columns \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m columns\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "num_samples_to_take = 8600\n",
    "ds = Dataset.from_pandas(bert_test_data, \"en\", split=\"train\")\n",
    "ds = ds.take(num_samples_to_take)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RajBu\\AppData\\Local\\Temp\\ipykernel_2072\\1199333552.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the model to train\n",
    "    args=training_args,                  # training arguments\n",
    "    train_dataset=bert_train_data,            # training dataset\n",
    "    eval_dataset=bert_val_data,               # evaluation dataset\n",
    "    tokenizer=tokenizer,                 # tokenizer\n",
    "    compute_metrics=lambda p: {\n",
    "        'accuracy': accuracy_score(p.predictions.argmax(axis=1), p.label_ids)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='254' max='2851' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 254/2851 18:33 < 3:11:17, 0.23 it/s, Epoch 0.09/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.635400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.563300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.508100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.490200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.485400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.399700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.435300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.384300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.427400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.438000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.462800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.460200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.349800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.405100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.328000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.326200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.327800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.272200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.447000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.254000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.352000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.292000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.361600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.231400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.253200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\trainer.py:2548\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2541\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2542\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2544\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2546\u001b[0m )\n\u001b[0;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2554\u001b[0m ):\n\u001b[0;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\trainer.py:3698\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3695\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3697\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3698\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3700\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3702\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3703\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3704\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\trainer.py:3759\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3757\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[0;32m   3758\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[1;32m-> 3759\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3760\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3761\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1673\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1665\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1666\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1667\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1668\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1669\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1670\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1671\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1673\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1674\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1679\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1680\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1681\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1683\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1685\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1687\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    686\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m         output_attentions,\n\u001b[0;32m    693\u001b[0m     )\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    575\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    584\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 585\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    592\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:515\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    507\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 515\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:408\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    406\u001b[0m     key_layer, value_layer \u001b[38;5;241m=\u001b[39m past_key_value\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 408\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    409\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(current_states))\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 07:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: {'eval_loss': 0.24607114493846893, 'eval_accuracy': 0.901, 'eval_runtime': 454.385, 'eval_samples_per_second': 4.402, 'eval_steps_per_second': 0.275, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate()\n",
    "print(f\"Results: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m predictions, labels, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(labels, predictions))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\trainer.py:4183\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[1;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4180\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   4182\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 4183\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPrediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\n\u001b[0;32m   4185\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4186\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   4187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\trainer.py:4289\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4286\u001b[0m observed_num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   4288\u001b[0m \u001b[38;5;66;03m# Main evaluation loop\u001b[39;00m\n\u001b[1;32m-> 4289\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   4290\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Update the observed num examples\u001b[39;49;00m\n\u001b[0;32m   4291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfind_batch_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4292\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\accelerate\\data_loader.py:564\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 564\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    566\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(bert_test_data)\n",
    "predictions = predictions.argmax(axis=1)\n",
    "\n",
    "print(classification_report(labels, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
