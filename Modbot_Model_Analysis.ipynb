{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using vader, roberta pretrained model and hugging face pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data and NLTK basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\RajBu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\RajBu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\RajBu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "import random\n",
    "import nltk\n",
    "nltk.download('punkt')  # Ensure the standard punkt tokenizer is installed\n",
    "nltk.download('averaged_perceptron_tagger')  # Sometimes required for tokenization\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45615, 2)\n",
      "                                                  text  label\n",
      "0    \"QT @user In the original draft of the 7th boo...      2\n",
      "1    \"Ben Smith / Smith (concussion) remains out of...      1\n",
      "2    Sorry bout the stream last night I crashed out...      1\n",
      "3    Chase Headley's RBI double in the 8th inning o...      1\n",
      "4    @user Alciato: Bee will invest 150 million in ...      2\n",
      "..                                                 ...    ...\n",
      "495  \"Your 3rd gen. iPad with Retina display is wor...      1\n",
      "496  \"From the Twitter just now from RNDM: \"\"\"\"RNDM...      1\n",
      "497  @user @user You are a Patriot! Such compassion...      2\n",
      "498  \"Watching Les Mis 25th Anniversary concert. Ho...      0\n",
      "499  I\\u2019d want Mannone\\u002cJenks/sagna\\u002cpe...      1\n",
      "\n",
      "[500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('sentiment_train.csv')\n",
    "print(df.shape)\n",
    "print(df.head(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = df['label'].value_counts().sort_index() \\\n",
    "    .plot(kind='bar',\n",
    "          title='Count of hate or not',\n",
    "          figsize=(10, 5))\n",
    "ax.set_xlabel('Hate Yay or Nay')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Little blurry but this is the crowd I play for every Thursday at Plan B with Madison\\u2019s hottest DJ\\u002c Brook Bartels\n"
     ]
    }
   ],
   "source": [
    "example = df['text'][random.randint(0, 500)]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Little', 'blurry', 'but', 'this', 'is', 'the', 'crowd', 'I', 'play', 'for']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(example)\n",
    "print(tokens[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Little', 'JJ'),\n",
       " ('blurry', 'NN'),\n",
       " ('but', 'CC'),\n",
       " ('this', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('crowd', 'NN'),\n",
       " ('I', 'PRP'),\n",
       " ('play', 'VBP'),\n",
       " ('for', 'IN')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(tokens)\n",
    "tagged[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Little/JJ)\n",
      "  blurry/NN\n",
      "  but/CC\n",
      "  this/DT\n",
      "  is/VBZ\n",
      "  the/DT\n",
      "  crowd/NN\n",
      "  I/PRP\n",
      "  play/VBP\n",
      "  for/IN\n",
      "  every/DT\n",
      "  Thursday/NNP\n",
      "  at/IN\n",
      "  (FACILITY Plan/NNP B/NNP)\n",
      "  with/IN\n",
      "  Madison\\u2019s/NNP\n",
      "  hottest/JJS\n",
      "  DJ\\u002c/NNP\n",
      "  (PERSON Brook/NNP Bartels/NNP))\n"
     ]
    }
   ],
   "source": [
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "entities.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "import contractions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hyperlinks_marks_styles(text):\n",
    "    new_text = re.sub(r'^RT[\\s]', '', text)\n",
    "    new_text = re.sub(r'@\\S+', '', new_text)\n",
    "    new_text = re.sub(r'https?:\\/\\/.*[\\r\\n]*','', new_text)\n",
    "    new_text = re.sub(r'#', '', new_text)\n",
    "\n",
    "    return new_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "\n",
    "    text_tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    return text_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\RajBu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "stopwords_english = stopwords.words('english')\n",
    "\n",
    "punctuations = string.punctuation\n",
    "\n",
    "def remove_punctuations_stopwords(text_tokens):\n",
    "\n",
    "    text_clean = []\n",
    "\n",
    "    for word in text_tokens:\n",
    "        if (word not in stopwords_english and word not in punctuations):\n",
    "            text_clean.append(word)\n",
    "\n",
    "    return text_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions_in_text(text):\n",
    "    return contractions.fix(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map NLTK POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ  # Adjective\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB  # Verb\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN  # Noun\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV  # Adverb\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text_tokens):\n",
    "    pos_tags = pos_tag(text_tokens)  # POS tagging\n",
    "    return [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = remove_hyperlinks_marks_styles(text)  # Remove unnecessary elements\n",
    "    text = expand_contractions_in_text(text)\n",
    "    tokens = tokenize_text(text)  # Tokenize\n",
    "    tokens = remove_punctuations_stopwords(tokens)  # Remove stopwords & punctuation\n",
    "    tokens = lemmatize_text(tokens)  # Lemmatize with POS tagging\n",
    "    return \" \".join(tokens)  # Convert back to string if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Oh just drinking a Westy on a Monday night listening to some Nirvana. #maternityleave\n",
      "Processed: Oh drinking Westy Monday night listen Nirvana maternityleave\n"
     ]
    }
   ],
   "source": [
    "text = df['text'][random.randint(0, 500)]\n",
    "processed_text = preprocess_text(text)\n",
    "\n",
    "print(\"Original:\", text)\n",
    "print(\"Processed:\", processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('sentiment_train.csv')\n",
    "val_data = pd.read_csv('sentiment_validation.csv')\n",
    "test_data = pd.read_csv('sentiment_test.csv')\n",
    "\n",
    "\n",
    "X_train, y_train = train_data['text'], train_data['label']\n",
    "X_val, y_val = val_data['text'], val_data['label']\n",
    "X_test, y_test = test_data['text'], test_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the text column is a string and handle NaNs\n",
    "X_train = X_train.astype(str).fillna('')\n",
    "X_val = X_val.astype(str).fillna('')\n",
    "X_test = X_test.astype(str).fillna('')\n",
    "\n",
    "X_train = X_train.apply(preprocess_text)\n",
    "X_val = X_val.apply(preprocess_text)\n",
    "X_test = X_test.apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        `` QT In original draft 7th book Remus Lupin s...\n",
      "1        `` Ben Smith Smith concussion remain lineup Th...\n",
      "2        Sorry bout stream last night I crash tonight s...\n",
      "3        Chase Headley 's RBI double 8th inning David P...\n",
      "4        Alciato Bee invest 150 million January another...\n",
      "                               ...                        \n",
      "45610    '' '' So amazing beautiful Lady Gaga show AC t...\n",
      "45611    9 September arrive mean Apple 's new iPhone ho...\n",
      "45612    Leeds 1-1 Sheff Wed. Giuseppe Bellusci secure ...\n",
      "45613    I hilton head till 8th lol go Jason aldean sep...\n",
      "45614    WASHINGTON Reuters YOU.S Vice President Joe Bi...\n",
      "Name: text, Length: 45615, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting text into numerical features using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=2, max_df=0.8) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on training data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform validation and test data\n",
    "X_val_tfidf = vectorizer.transform(X_val)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_recall_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converting labels to negative or non negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_binary = y_train.apply(lambda x: 1 if x == 0 else 0)  # 0 = negative (target), 1 & 2 = non-negative\n",
    "y_val_binary = y_val.apply(lambda x: 1 if x == 0 else 0)\n",
    "y_test_binary = y_test.apply(lambda x: 1 if x == 0 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 0.5, 1.0, 1.5,2.0, 5.0, 7.5, 10],  # Smoothing parameter\n",
    "    'fit_prior': [True, False]      # Whether to learn class prior probabilities\n",
    "}\n",
    "\n",
    "# Track the best model and score\n",
    "best_score = 0\n",
    "best_params = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Accuracy: 0.8565\n",
      "Best Hyperparameters: {'alpha': 0.1, 'fit_prior': True}\n"
     ]
    }
   ],
   "source": [
    "for alpha in param_grid['alpha']:\n",
    "    for fit_prior in param_grid['fit_prior']:\n",
    "        # Set model parameters\n",
    "        model.set_params(alpha=alpha, fit_prior=fit_prior)\n",
    "        \n",
    "        # Train on the training set\n",
    "        model.fit(X_train_tfidf, y_train_binary)\n",
    "        \n",
    "        # Evaluate on the validation set\n",
    "        val_predictions = model.predict(X_val_tfidf)\n",
    "        val_score = accuracy_score(y_val_binary, val_predictions)\n",
    "        \n",
    "        # Update best model if needed\n",
    "        if val_score > best_score:\n",
    "            best_score = val_score\n",
    "            best_params = {'alpha': alpha, 'fit_prior': fit_prior}\n",
    "\n",
    "print(f\"Best Validation Accuracy: {best_score}\")\n",
    "print(f\"Best Hyperparameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6769781830022794\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.99      0.81      8312\n",
      "           1       0.51      0.03      0.06      3972\n",
      "\n",
      "    accuracy                           0.68     12284\n",
      "   macro avg       0.59      0.51      0.43     12284\n",
      "weighted avg       0.62      0.68      0.56     12284\n",
      "\n",
      "Confusion Matrix:\n",
      " [[8198  114]\n",
      " [3854  118]]\n",
      "F1 Score: 0.056137012369172214\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on Test Set\n",
    "y_test_pred = model.predict(X_test_tfidf)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test_binary, y_test_pred))\n",
    "print(\"Test Classification Report:\\n\", classification_report(y_test_binary, y_test_pred))\n",
    "\n",
    "cm = confusion_matrix(y_test_binary, y_test_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "\n",
    "f1 = f1_score(y_test_binary, y_test_pred)\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nn_preprocess_text(text, remove_stopwords=True):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove hyperlinks etc.\n",
    "\n",
    "    text = remove_hyperlinks_marks_styles(text)\n",
    "\n",
    "    text = expand_contractions_in_text(text)\n",
    "\n",
    "    # Remove special characters, numbers, and punctuation\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "    \n",
    "    #Remove stopwords\n",
    "    if remove_stopwords:\n",
    "        text = ' '.join(word for word in text.split() if word not in stopwords_english)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Gonna go to the north game tomorrow night. I better see some of you north kiddos\n",
      "Processed: going go north game tomorrow night better see north kiddos\n"
     ]
    }
   ],
   "source": [
    "text = df['text'][random.randint(0, 500)]\n",
    "processed_text = nn_preprocess_text(text)\n",
    "\n",
    "print(\"Original:\", text)\n",
    "print(\"Processed:\", processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_train_data = pd.read_csv('sentiment_train.csv')\n",
    "nn_val_data = pd.read_csv('sentiment_validation.csv')\n",
    "nn_test_data = pd.read_csv('sentiment_test.csv')\n",
    "\n",
    "\n",
    "nn_X_train, nn_y_train = nn_train_data['text'], nn_train_data['label']\n",
    "nn_X_val, nn_y_val = nn_val_data['text'], nn_val_data['label']\n",
    "nn_X_test, nn_y_test = nn_test_data['text'], nn_test_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the text column is a string and handle NaNs\n",
    "nn_X_train = nn_X_train.astype(str).fillna('')\n",
    "nn_X_val = nn_X_val.astype(str).fillna('')\n",
    "nn_X_test = nn_X_test.astype(str).fillna('')\n",
    "\n",
    "nn_X_train = nn_X_train.apply(nn_preprocess_text)\n",
    "nn_X_val = nn_X_val.apply(nn_preprocess_text)\n",
    "nn_X_test = nn_X_test.apply(nn_preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of 0        qt original draft th book remus lupin survived...\n",
      "1        ben smith smith concussion remains lineup thur...\n",
      "2        sorry bout stream last night crashed tonight s...\n",
      "3        chase headleys rbi double th inning david pric...\n",
      "4        alciato bee invest million january another sum...\n",
      "                               ...                        \n",
      "45610    amazing beautiful lady gaga show ac tonight lo...\n",
      "45611    september ha arrived mean apple new iphone hou...\n",
      "45612    leeds sheff wed giuseppe bellusci securing luf...\n",
      "45613    hilton head till th lol go jason aldean sept t...\n",
      "45614    washington reuters yous vice president joe bid...\n",
      "Name: text, Length: 45615, dtype: object>\n"
     ]
    }
   ],
   "source": [
    "print(nn_X_train.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "nn_X_train_seq = tokenizer.texts_to_sequences(nn_X_train)\n",
    "nn_X_val_seq = tokenizer.texts_to_sequences(nn_X_val)\n",
    "nn_X_test_seq = tokenizer.texts_to_sequences(nn_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding sequences\n",
    "max_sequence_length = 100\n",
    "\n",
    "nn_X_train_padded = pad_sequences(nn_X_train_seq, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "nn_X_val_padded = pad_sequences(nn_X_val_seq, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "nn_X_test_padded = pad_sequences(nn_X_test_seq, maxlen=max_sequence_length, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converting labels to negative or non negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_y_train_binary = nn_y_train.apply(lambda x: 1 if x == 0 else 0)  # 0 = negative (target), 1 & 2 = non-negative\n",
    "nn_y_val_binary = nn_y_val.apply(lambda x: 1 if x == 0 else 0)\n",
    "nn_y_test_binary = nn_y_test.apply(lambda x: 1 if x == 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=100),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    Dropout(0.5),\n",
    "    LSTM(32),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m1426/1426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 41ms/step - accuracy: 0.8434 - loss: 0.4457 - val_accuracy: 0.8440 - val_loss: 0.4337\n",
      "Epoch 2/15\n",
      "\u001b[1m1426/1426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 40ms/step - accuracy: 0.8416 - loss: 0.4433 - val_accuracy: 0.8440 - val_loss: 0.4334\n",
      "Epoch 3/15\n",
      "\u001b[1m1426/1426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 40ms/step - accuracy: 0.8432 - loss: 0.4388 - val_accuracy: 0.8440 - val_loss: 0.4331\n",
      "Epoch 4/15\n",
      "\u001b[1m1426/1426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 41ms/step - accuracy: 0.8438 - loss: 0.4368 - val_accuracy: 0.8440 - val_loss: 0.4330\n",
      "Epoch 5/15\n",
      "\u001b[1m1426/1426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 40ms/step - accuracy: 0.8447 - loss: 0.4352 - val_accuracy: 0.8440 - val_loss: 0.4330\n",
      "Epoch 6/15\n",
      "\u001b[1m1426/1426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 41ms/step - accuracy: 0.8410 - loss: 0.4405 - val_accuracy: 0.8440 - val_loss: 0.4334\n",
      "Epoch 7/15\n",
      "\u001b[1m1426/1426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 41ms/step - accuracy: 0.8466 - loss: 0.4308 - val_accuracy: 0.8440 - val_loss: 0.4330\n",
      "Epoch 8/15\n",
      "\u001b[1m1426/1426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 41ms/step - accuracy: 0.8475 - loss: 0.4288 - val_accuracy: 0.8440 - val_loss: 0.4331\n",
      "Epoch 9/15\n",
      "\u001b[1m1426/1426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 41ms/step - accuracy: 0.8449 - loss: 0.4336 - val_accuracy: 0.8440 - val_loss: 0.4330\n",
      "Epoch 10/15\n",
      "\u001b[1m1426/1426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 41ms/step - accuracy: 0.8458 - loss: 0.4314 - val_accuracy: 0.8440 - val_loss: 0.4330\n",
      "Epoch 11/15\n",
      "\u001b[1m1426/1426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 41ms/step - accuracy: 0.8453 - loss: 0.4322 - val_accuracy: 0.8440 - val_loss: 0.4331\n",
      "Epoch 12/15\n",
      "\u001b[1m1426/1426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 78ms/step - accuracy: 0.8446 - loss: 0.4333 - val_accuracy: 0.8440 - val_loss: 0.4330\n",
      "Epoch 13/15\n",
      "\u001b[1m1426/1426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 40ms/step - accuracy: 0.8412 - loss: 0.4387 - val_accuracy: 0.8440 - val_loss: 0.4330\n",
      "Epoch 14/15\n",
      "\u001b[1m1426/1426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 42ms/step - accuracy: 0.8479 - loss: 0.4271 - val_accuracy: 0.8440 - val_loss: 0.4331\n",
      "Epoch 15/15\n",
      "\u001b[1m1426/1426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 41ms/step - accuracy: 0.8425 - loss: 0.4362 - val_accuracy: 0.8440 - val_loss: 0.4331\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2809372d290>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "model.fit(nn_X_train_padded, nn_y_train_binary, validation_data=(nn_X_val_padded, nn_y_val_binary), epochs=15, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      1.00      0.81      8312\n",
      "           1       0.00      0.00      0.00      3972\n",
      "\n",
      "    accuracy                           0.68     12284\n",
      "   macro avg       0.34      0.50      0.40     12284\n",
      "weighted avg       0.46      0.68      0.55     12284\n",
      "\n",
      "[[8312    0]\n",
      " [3972    0]]\n",
      "Accuracy: 0.6766525561706285\n",
      "F1 Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RajBu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\RajBu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\RajBu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "nn_y_pred = (model.predict(nn_X_test_padded) > 0.5).astype('int32')\n",
    "print(classification_report(nn_y_test_binary, nn_y_pred))\n",
    "print(confusion_matrix(nn_y_test_binary, nn_y_pred))\n",
    "print('Accuracy:', accuracy_score(nn_y_test_binary, nn_y_pred))\n",
    "\n",
    "nn_f1 = f1_score(nn_y_test_binary, nn_y_pred)\n",
    "print(f\"F1 Score: {nn_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RajBu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\RajBu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_train_data = pd.read_csv('sentiment_train.csv')\n",
    "bert_val_data = pd.read_csv('sentiment_validation.csv')\n",
    "bert_test_data = pd.read_csv('sentiment_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.cuda' has no attribute 'device_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())  \u001b[38;5;66;03m# Should return True if GPU is available\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m(\u001b[38;5;241m0\u001b[39m))  \u001b[38;5;66;03m# Should show your GPU model\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch.cuda' has no attribute 'device_name'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_train_data['label'] = bert_train_data['label'].apply(lambda x: 1 if x != 0 else 0)\n",
    "bert_val_data['label'] = bert_val_data['label'].apply(lambda x: 1 if x != 0 else 0)\n",
    "bert_test_data['label'] = bert_test_data['label'].apply(lambda x: 1 if x != 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"QT @user In the original draft of the 7th boo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Ben Smith / Smith (concussion) remains out of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sorry bout the stream last night I crashed out...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chase Headley's RBI double in the 8th inning o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@user Alciato: Bee will invest 150 million in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>@user I\\u2019m sick with something ill be at s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>\"There's something about Friday Night Lights, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Googled the snake I stepped over on the trail ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>@user Also, his anger against Hindus are justi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Ricky Ponting and I now have something in comm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  label\n",
       "0   \"QT @user In the original draft of the 7th boo...      1\n",
       "1   \"Ben Smith / Smith (concussion) remains out of...      1\n",
       "2   Sorry bout the stream last night I crashed out...      1\n",
       "3   Chase Headley's RBI double in the 8th inning o...      1\n",
       "4   @user Alciato: Bee will invest 150 million in ...      1\n",
       "..                                                ...    ...\n",
       "95  @user I\\u2019m sick with something ill be at s...      0\n",
       "96  \"There's something about Friday Night Lights, ...      1\n",
       "97  Googled the snake I stepped over on the trail ...      1\n",
       "98  @user Also, his anger against Hindus are justi...      0\n",
       "99  Ricky Ponting and I now have something in comm...      1\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_train_data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_train_data = Dataset.from_pandas(bert_train_data)\n",
    "bert_val_data = Dataset.from_pandas(bert_val_data)\n",
    "bert_test_data = Dataset.from_pandas(bert_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 45615/45615 [00:24<00:00, 1846.73 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:01<00:00, 1752.61 examples/s]\n",
      "Map: 100%|██████████| 12284/12284 [00:06<00:00, 1870.79 examples/s]\n"
     ]
    }
   ],
   "source": [
    "bert_train_data = bert_train_data.map(tokenize_function, batched=True)\n",
    "bert_val_data = bert_val_data.map(tokenize_function, batched=True)\n",
    "bert_test_data = bert_test_data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_train_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "bert_val_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "bert_test_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=1,              # training epochs\n",
    "    per_device_train_batch_size=16,   # batch size for training\n",
    "    per_device_eval_batch_size=16,    # batch size for evaluation\n",
    "    warmup_steps=500,                # warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RajBu\\AppData\\Local\\Temp\\ipykernel_14548\\1199333552.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the model to train\n",
    "    args=training_args,                  # training arguments\n",
    "    train_dataset=bert_train_data,            # training dataset\n",
    "    eval_dataset=bert_val_data,               # evaluation dataset\n",
    "    tokenizer=tokenizer,                 # tokenizer\n",
    "    compute_metrics=lambda p: {\n",
    "        'accuracy': accuracy_score(p.predictions.argmax(axis=1), p.label_ids)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='2851' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  17/2851 00:52 < 2:45:35, 0.29 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.661900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\trainer.py:2548\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2541\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2542\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2544\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2546\u001b[0m )\n\u001b[0;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2554\u001b[0m ):\n\u001b[0;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\trainer.py:3740\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[0;32m   3738\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 3740\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3742\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\accelerate\\accelerator.py:2329\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2328\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2329\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate()\n",
    "print(f\"Results: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels, _ = trainer.predict(test_data)\n",
    "predictions = predictions.argmax(axis=1)\n",
    "\n",
    "print(classification_report(labels, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
